from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import json
import time
import re
import os
import argparse
from datetime import datetime
import sys
from tqdm import tqdm
from parser import NaraParser, DataExporter
from metadata_openapi import OpenAPIMetadataScanner
import concurrent.futures
import threading
import queue
import psutil
import gc
from selenium.webdriver.chrome.service import Service
from selenium.common.exceptions import TimeoutException, WebDriverException

# ì „ì—­ ë³€ìˆ˜
driver_pool = None

class OptimizedDriverPool:
    """ìµœì í™”ëœ WebDriver í’€ ê´€ë¦¬ í´ë˜ìŠ¤"""
    def __init__(self, pool_size=10):
        self.pool_size = pool_size
        self.drivers = queue.Queue(maxsize=pool_size)
        self.lock = threading.Lock()
        self._initialize_pool()
    
    def _initialize_pool(self):
        """ë“œë¼ì´ë²„ í’€ ì´ˆê¸°í™”"""
        for _ in range(self.pool_size):
            driver = self._create_driver()
            self.drivers.put(driver)
    
    def _create_driver(self):
        """ìµœì í™”ëœ WebDriver ìƒì„±"""
        opts = Options()
        opts.add_argument("--headless")
        opts.add_argument("--no-sandbox")
        opts.add_argument("--disable-dev-shm-usage")
        opts.add_argument("--disable-gpu")
        opts.add_argument("--disable-extensions")
        opts.add_argument("--disable-plugins")
        opts.add_argument("--disable-background-timer-throttling")
        opts.add_argument("--disable-renderer-backgrounding")
        opts.add_argument("--disable-backgrounding-occluded-windows")
        opts.add_argument("--disable-features=TranslateUI")
        opts.add_argument("--disable-ipc-flooding-protection")
        opts.add_argument("--disable-images")
        opts.add_argument("--disable-css")
        opts.add_argument("--log-level=3")
        opts.add_argument("--no-default-browser-check")
        opts.add_argument("--no-first-run")
        opts.add_argument("--disable-default-apps")
        
        opts.add_experimental_option('excludeSwitches', ['enable-logging'])
        opts.add_experimental_option('useAutomationExtension', False)
        
        return webdriver.Chrome(options=opts)
    
    def get_driver(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë“œë¼ì´ë²„ ë°˜í™˜"""
        try:
            return self.drivers.get(timeout=5)
        except queue.Empty:
            return self._create_driver()
    
    def return_driver(self, driver):
        """ë“œë¼ì´ë²„ë¥¼ í’€ì— ë°˜í™˜"""
        try:
            driver.get("about:blank")
            driver.delete_all_cookies()
            if self.drivers.qsize() < self.pool_size:
                self.drivers.put(driver)
            else:
                driver.quit()
        except:
            try:
                driver.quit()
            except:
                pass
    
    def close_all(self):
        """ëª¨ë“  ë“œë¼ì´ë²„ ì¢…ë£Œ"""
        while not self.drivers.empty():
            try:
                driver = self.drivers.get_nowait()
                driver.quit()
            except:
                pass

class MemoryManager:
    """ë©”ëª¨ë¦¬ ê´€ë¦¬ í´ë˜ìŠ¤"""
    @staticmethod
    def get_memory_usage():
        """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë°˜í™˜"""
        process = psutil.Process()
        return process.memory_info().rss / 1024 / 1024  # MB ë‹¨ìœ„
    
    @staticmethod
    def check_memory_threshold(threshold_mb=2000):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì„ê³„ê°’ì„ ì´ˆê³¼í•˜ëŠ”ì§€ í™•ì¸"""
        return MemoryManager.get_memory_usage() > threshold_mb
    
    @staticmethod
    def cleanup():
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        gc.collect()

def get_api_id(url):
    """URLì—ì„œ API ID ì¶”ì¶œ"""
    m = re.search(r'/data/(\d+)/openapi', url)
    return m.group(1) if m else f"api_{url.replace('https://', '').replace('/', '_')}"

def crawl_url(url, output_dir, formats, driver_pool, timing_results=None):
    """ë‹¨ì¼ URL í¬ë¡¤ë§ - ê°œì„ ëœ ìˆœì„œ"""
    if timing_results is not None:
        start_time = time.time()
    os.makedirs(output_dir, exist_ok=True)
    api_id = get_api_id(url)
    
    driver = driver_pool.get_driver()
    
    crawling_result = {
        'success': False,
        'data': None,
        'saved_files': [],
        'errors': [],
        'api_id': api_id,
        'url': url
    }
    
    try:
        driver.get(url)
        
        # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
        WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.TAG_NAME, "body")))
        
        parser = NaraParser(driver)
        
        # ê²°ê³¼ ë°ì´í„° ì´ˆê¸°í™”
        result = {
            'api_id': api_id,
            'crawled_url': url,
            'crawled_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
        
        # Step 1: ìš°ì„ ì ìœ¼ë¡œ í…Œì´ë¸” ì •ë³´ ì¶”ì¶œ
        table_info = parser.extract_table_info()
        result['info'] = table_info
        
        # API ìœ í˜• í™•ì¸
        api_type_field = table_info.get('API ìœ í˜•', '').upper()
        
        # Step 2: API ìœ í˜•ì´ LINKì¸ ê²½ìš° í…Œì´ë¸” í¬ë¡¤ë§ë§Œ í•˜ê³  ì¢…ë£Œ
        if 'LINK' in api_type_field:
            result['api_type'] = 'link'
            result['skip_reason'] = 'LINK íƒ€ì… APIëŠ” í…Œì´ë¸” ì •ë³´ë§Œ ìˆ˜ì§‘'
            
            crawling_result['data'] = result
            crawling_result['success'] = True
            
            # ë°ì´í„° ì €ì¥
            saved_files, save_errors = DataExporter.save_crawling_result(result, output_dir, api_id, formats)
            
            crawling_result['saved_files'] = saved_files
            crawling_result['errors'] = save_errors
            
            if timing_results is not None:
                timing_results[url] = time.time() - start_time
            return crawling_result['success']
        
        # Step 3: REST APIì¸ ê²½ìš° Swagger JSON ì¶”ì¶œ ì‹œë„
        swagger_json = parser.extract_swagger_json()
        
        if swagger_json and isinstance(swagger_json, dict) and swagger_json:
            # Swagger JSONì´ ì¡´ì¬í•˜ëŠ” ê²½ìš°
            # API ì •ë³´ ì¶”ì¶œ
            api_info = parser.extract_api_info(swagger_json)
            
            # Base URL ì¶”ì¶œ
            base_url = parser.extract_base_url(swagger_json)
            api_info['base_url'] = base_url
            
            # Schemes ì¶”ì¶œ
            api_info['schemes'] = swagger_json.get('schemes', ['https'])
            
            # ì—”ë“œí¬ì¸íŠ¸ ì¶”ì¶œ
            endpoints = parser.extract_endpoints(swagger_json)
            
            # ê²°ê³¼ ë°ì´í„° êµ¬ì„±
            result.update({
                'api_info': api_info,
                'endpoints': endpoints,
                'swagger_json': swagger_json,
                'api_type': 'swagger'
            })
            
        else:
            # Step 4: Swagger JSONì´ ì—†ëŠ” ê²½ìš° ì¼ë°˜ API ì •ë³´ ì¶”ì¶œ
            general_api_info = parser.extract_general_api_info()
            
            if general_api_info and (general_api_info.get('detail_info') or 
                                   general_api_info.get('request_parameters') or 
                                   general_api_info.get('response_elements')):
                # ê²°ê³¼ ë°ì´í„° êµ¬ì„±
                result.update({
                    'general_api_info': general_api_info,
                    'api_type': 'general'
                })
            else:
                # ì •ë³´ë¶€ì¡± URLì„ ë³„ë„ íŒŒì¼ì— ì €ì¥
                failed_urls_file = os.path.join(output_dir, "failed_urls.txt")
                os.makedirs(output_dir, exist_ok=True)
                with open(failed_urls_file, 'a', encoding='utf-8') as f:
                    f.write(f"{url}\n")
                
                crawling_result['errors'].append("API ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ failed_urls.txtì— ê¸°ë¡")
                if timing_results is not None:
                    timing_results[url] = time.time() - start_time
                return False
        
        crawling_result['data'] = result
        crawling_result['success'] = True
        
        # ë°ì´í„° ì €ì¥
        saved_files, save_errors = DataExporter.save_crawling_result(result, output_dir, api_id, formats)
        
        crawling_result['saved_files'] = saved_files
        crawling_result['errors'] = save_errors
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        MemoryManager.cleanup()
        if timing_results is not None:
            timing_results[url] = time.time() - start_time
        return crawling_result['success']
    
    except Exception as e:
        error_msg = f"í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}"
        print(f"âŒ {error_msg}")
        crawling_result['errors'].append(error_msg)
        if timing_results is not None:
            timing_results[url] = time.time() - start_time
        return False
    finally:
        driver_pool.return_driver(driver)

def generate_urls_from_numbers(numbers):
    """ìˆ«ì ë¦¬ìŠ¤íŠ¸ì—ì„œ URL ìƒì„±"""
    base_url = "https://www.data.go.kr/data/{}/openapi.do"
    return [base_url.format(num) for num in numbers]

def generate_urls(start_num, end_num):
    """ì‹œì‘ë²ˆí˜¸ì™€ ëë²ˆí˜¸ ì‚¬ì´ì˜ ëª¨ë“  URL ìƒì„±"""
    base_url = "https://www.data.go.kr/data/{}/openapi.do"
    return [base_url.format(num) for num in range(start_num, end_num + 1)]

def check_metadata_and_get_valid_numbers(start_num, end_num):
    """ë©”íƒ€ë°ì´í„°ë¥¼ ì²´í¬í•˜ì—¬ ìœ íš¨í•œ ë²ˆí˜¸ë“¤ë§Œ ë°˜í™˜"""
    print(f"\nğŸ” ë©”íƒ€ë°ì´í„° ìŠ¤ìº” ì‹œì‘: {start_num} ~ {end_num}")
    
    # ë©”íƒ€ë°ì´í„° ìŠ¤ìºë„ˆ ìƒì„±
    scanner = OpenAPIMetadataScanner(
        start_num=start_num,
        end_num=end_num,
        max_workers=50
    )
    
    # ë©”íƒ€ë°ì´í„° ìŠ¤ìº” ì‹¤í–‰
    results = scanner.scan_range()
    
    # ê²°ê³¼ ì €ì¥
    scanner.save_results()
    
    # ìš”ì•½ ì¶œë ¥
    scanner.print_summary()
    
    # íŒŒì¼ ë°ì´í„°ê°€ ìˆëŠ” ë²ˆí˜¸ë“¤ ë°˜í™˜
    valid_numbers = results['file_numbers']
    
    print(f"\nâœ… ë©”íƒ€ë°ì´í„° ìŠ¤ìº” ì™„ë£Œ!")
    print(f"   ğŸ“‹ ì „ì²´ ìŠ¤ìº”: {results['total']}ê°œ")
    print(f"   âœ… ìœ íš¨í•œ ë²ˆí˜¸: {len(valid_numbers)}ê°œ")
    print(f"   ğŸ“Š í•„í„°ë§ ë¹„ìœ¨: {(len(valid_numbers) / results['total'] * 100):.1f}%")
    
    return valid_numbers

def batch_crawl(urls, output_dir="/data/download_openapi", formats=['json', 'xml', 'md', 'csv'], max_workers=40):
    """ë²”ìœ„ ë‚´ì˜ ëª¨ë“  API ë¬¸ì„œ í¬ë¡¤ë§"""
    total_urls = len(urls)
    
    print(f"\nğŸš€ ë°°ì¹˜ í¬ë¡¤ë§ ì‹œì‘")
    print(f"   ğŸ“‹ ì´ {total_urls}ê°œ URL")
    print(f"   ğŸ‘¥ ë™ì‹œ ì‘ì—…ì: {max_workers}ê°œ")
    print(f"   ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
    print(f"   ğŸ’¾ ì €ì¥ í˜•ì‹: {', '.join(formats)}")
    
    # ë“œë¼ì´ë²„ í’€ ì´ˆê¸°í™”
    driver_pool = OptimizedDriverPool(pool_size=max_workers)
    
    # ê²°ê³¼ ì €ì¥ìš© ë³€ìˆ˜
    results = {
        'total': total_urls,
        'success': 0,
        'failed': 0,
        'link_type': 0,
        'swagger_type': 0,
        'general_type': 0,
        'insufficient_info': 0,  # ì •ë³´ë¶€ì¡± ì¹´ìš´íŠ¸ ì¶”ê°€
        'failed_urls': [],
        'start_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    }
    
    timing_results = {}
    
    try:
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_url = {
                executor.submit(crawl_url, url, output_dir, formats, driver_pool, timing_results): url 
                for url in urls
            }
            
            with tqdm(concurrent.futures.as_completed(future_to_url), total=total_urls, desc="í¬ë¡¤ë§ ì§„í–‰") as pbar:
                for future in pbar:
                    url = future_to_url[future]
                    
                    if MemoryManager.check_memory_threshold():
                        print("\nâš ï¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë†’ìŠµë‹ˆë‹¤. ì •ë¦¬ ì¤‘...")
                        MemoryManager.cleanup()
                    
                    try:
                        result = future.result()
                        if result:
                            results['success'] += 1
                        else:
                            results['failed'] += 1
                    except Exception as e:
                        results['failed'] += 1
                        results['failed_urls'].append(url)
                        print(f"\nâš ï¸ ì˜ˆì™¸ ë°œìƒ: {url} - {str(e)}")
                    
                    pbar.update(0)
                    
                    if timing_results:
                        avg_time = sum(timing_results.values()) / len(timing_results)
                        pbar.set_postfix({'í‰ê· ì†Œìš”(s)': f'{avg_time:.1f}'})
                    if pbar.n % 10 == 0:
                        MemoryManager.cleanup()
    
    finally:
        driver_pool.close_all()
    
    # ê²°ê³¼ ìš”ì•½
    results['end_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    results['success_rate'] = f"{(results['success'] / total_urls * 100):.1f}%" if total_urls > 0 else "0%"
    results['timing_per_url'] = timing_results
    
    # ê²°ê³¼ ì €ì¥
    summary_file = os.path.join(output_dir, "crawling_summary.json")
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    # ì‹¤íŒ¨í•œ URL ëª©ë¡ ì €ì¥ (ì˜ˆì™¸ ë°œìƒí•œ URLë§Œ)
    if results['failed_urls']:
        exception_urls_file = os.path.join(output_dir, "exception_urls.txt")
        with open(exception_urls_file, 'w', encoding='utf-8') as f:
            for url in results['failed_urls']:
                f.write(f"{url}\n")
    
    # failed_urls.txt íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ (ì •ë³´ë¶€ì¡± URLë“¤)
    failed_urls_file = os.path.join(output_dir, "failed_urls.txt")
    insufficient_info_count = 0
    if os.path.exists(failed_urls_file):
        with open(failed_urls_file, 'r', encoding='utf-8') as f:
            insufficient_info_count = len(f.readlines())
        results['insufficient_info'] = insufficient_info_count
    
    # ìµœì¢… ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 50)
    print("ğŸ ë°°ì¹˜ í¬ë¡¤ë§ ì™„ë£Œ!")
    print("=" * 50)
    print(f"ğŸ“Š ì „ì²´ ê²°ê³¼:")
    print(f"   ğŸ“‹ ì´ ì²˜ë¦¬: {results['total']}ê°œ URL")
    print(f"   âœ… ì„±ê³µ: {results['success']}ê°œ ({results['success_rate']})")
    print(f"   âŒ ì‹¤íŒ¨: {results['failed']}ê°œ")
    if insufficient_info_count > 0:
        print(f"   ğŸ“ ì •ë³´ë¶€ì¡±: {insufficient_info_count}ê°œ (failed_urls.txtì— ê¸°ë¡)")
    print(f"   ğŸ“ ê²°ê³¼ ìœ„ì¹˜: {output_dir}")
    print(f"   ğŸ“‹ ìš”ì•½ íŒŒì¼: crawling_summary.json")
    if results['failed'] > 0:
        print(f"   ğŸ“„ ì˜ˆì™¸ ëª©ë¡: exception_urls.txt")
    if insufficient_info_count > 0:
        print(f"   ğŸ“„ ì •ë³´ë¶€ì¡± ëª©ë¡: failed_urls.txt")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='ë‚˜ë¼ì¥í„° API í¬ë¡¤ëŸ¬')
    parser.add_argument('-s', '--start', type=int, required=True, help='ì‹œì‘ ë¬¸ì„œ ë²ˆí˜¸')
    parser.add_argument('-e', '--end', type=int, required=True, help='ë ë¬¸ì„œ ë²ˆí˜¸')
    parser.add_argument('-o', '--output-dir', default='/data/download_openapi', help='ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: /data/download_openapi)')
    parser.add_argument('--formats', nargs='+', default=['json', 'xml', 'md', 'csv'],
                      choices=['json', 'xml', 'md', 'csv'], help='ì €ì¥í•  íŒŒì¼ í˜•ì‹')
    parser.add_argument('-w', '--workers', type=int, default=20, help='ë™ì‹œ ì‘ì—…ì ìˆ˜ (ê¸°ë³¸ê°’: 20)')
    parser.add_argument('--no-headless', action='store_true', help='í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ ë¹„í™œì„±í™”')
    parser.add_argument('--timeout', type=int, default=5, help='í˜ì´ì§€ ë¡œë“œ íƒ€ì„ì•„ì›ƒ (ì´ˆ)')
    parser.add_argument('--skip-metadata', action='store_true', help='ë©”íƒ€ë°ì´í„° ìŠ¤ìº” ê±´ë„ˆë›°ê¸° (ëª¨ë“  ë²ˆí˜¸ í¬ë¡¤ë§)')
    parser.add_argument('--scan-type', choices=['openapi'], default='openapi',
                      help='ë©”íƒ€ë°ì´í„° ìŠ¤ìº” íƒ€ì… (ê¸°ë³¸ê°’: openapi)')
    
    args = parser.parse_args()
    
    # ì…ë ¥ê°’ ê²€ì¦
    if args.start > args.end:
        print("âŒ ì‹œì‘ ë²ˆí˜¸ê°€ ë ë²ˆí˜¸ë³´ë‹¤ í´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        sys.exit(1)
    
    if args.workers < 10 or args.workers > 40:
        print("âš ï¸ ë™ì‹œ ì‘ì—…ì ìˆ˜ëŠ” 10-40 ì‚¬ì´ë¡œ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        args.workers = 20
    
    # ë©”íƒ€ë°ì´í„° ìŠ¤ìº” ì—¬ë¶€ì— ë”°ë¼ ì²˜ë¦¬
    if args.skip_metadata:
        print("âš ï¸ ë©”íƒ€ë°ì´í„° ìŠ¤ìº”ì„ ê±´ë„ˆë›°ê³  ëª¨ë“  ë²ˆí˜¸ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤.")
        urls = generate_urls(args.start, args.end)
    else:
        # ë©”íƒ€ë°ì´í„° ìŠ¤ìº”ì„ í†µí•´ ìœ íš¨í•œ ë²ˆí˜¸ë§Œ ì¶”ì¶œ
        valid_numbers = check_metadata_and_get_valid_numbers(
            args.start, args.end
        )
        
        if not valid_numbers:
            print("âŒ ìœ íš¨í•œ ë²ˆí˜¸ê°€ ì—†ìŠµë‹ˆë‹¤. í¬ë¡¤ë§ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            sys.exit(1)
        
        # ìœ íš¨í•œ ë²ˆí˜¸ë“¤ë¡œ URL ìƒì„±
        urls = generate_urls_from_numbers(valid_numbers)
    
    # ë°°ì¹˜ í¬ë¡¤ë§ ì‹¤í–‰
    batch_crawl(
        urls,
        args.output_dir,
        args.formats,
        args.workers
    )

if __name__ == '__main__':
    main()