"""
ë™ì  ì…€ë ‰í„° í…Œì´ë¸” í¬ë¡¤ëŸ¬
ì…€ë ‰íŠ¸ ë°•ìŠ¤ë¡œ ì¡°íšŒ ê°€ëŠ¥í•œ ë™ì  í…Œì´ë¸”ì„ í¬ë¡¤ë§
"""
import asyncio
from playwright.async_api import async_playwright, Page
from typing import Dict, List, Optional
import re
import json
from datetime import datetime

class DynamicTableCrawler:
    """ì…€ë ‰í„° ê¸°ë°˜ ë™ì  í…Œì´ë¸” í¬ë¡¤ëŸ¬"""

    def __init__(self):
        self.results = []

    @staticmethod
    def clean_text(text):
        """í…ìŠ¤íŠ¸ ì •ì œ"""
        if not isinstance(text, str):
            return text
        text = re.sub(r'[\n\r\t]+', ' ', text)
        text = re.sub(r' +', ' ', text)
        return text.strip()

    async def extract_selector_options(self, page: Page, selector_element) -> List[Dict]:
        """ì…€ë ‰í„° ì˜µì…˜ ì¶”ì¶œ"""
        options = []
        option_elements = await selector_element.query_selector_all('option')

        for option in option_elements:
            value = await option.get_attribute('value')
            text = await option.inner_text()

            # ë¹ˆ ê°’ì´ë‚˜ "ì„ íƒ" ë“±ì€ ì œì™¸
            if value and value.strip() and value != '':
                options.append({
                    'value': value.strip(),
                    'text': self.clean_text(text)
                })

        return options

    async def extract_table_data(self, page: Page, table_element) -> Dict:
        """í…Œì´ë¸” ë°ì´í„° ì¶”ì¶œ"""
        table_data = {
            'headers': [],
            'rows': []
        }

        # í—¤ë” ì¶”ì¶œ
        header_row = await table_element.query_selector('thead tr, tr:first-child')
        if header_row:
            headers = await header_row.query_selector_all('th, td')
            for header in headers:
                text = await header.inner_text()
                table_data['headers'].append(self.clean_text(text))

        # ë°ì´í„° í–‰ ì¶”ì¶œ
        body_rows = await table_element.query_selector_all('tbody tr, tr')

        for row in body_rows:
            # í—¤ë” í–‰ì€ ìŠ¤í‚µ
            if await row.query_selector('th') and not await row.query_selector('td'):
                continue

            cells = await row.query_selector_all('td, th')
            row_data = []

            for cell in cells:
                text = await cell.inner_text()
                row_data.append(self.clean_text(text))

            if row_data:  # ë¹ˆ í–‰ ì œì™¸
                table_data['rows'].append(row_data)

        return table_data

    async def crawl_dynamic_selector_table(
        self,
        url: str,
        selector_css: str = None,
        selector_xpath: str = None,
        table_container_css: str = None,
        table_container_xpath: str = None,
        wait_after_select: float = 2.0
    ) -> Dict:
        """
        ë™ì  ì…€ë ‰í„° í…Œì´ë¸” í¬ë¡¤ë§

        Args:
            url: í¬ë¡¤ë§í•  URL
            selector_css: ì…€ë ‰íŠ¸ ë°•ìŠ¤ CSS ì…€ë ‰í„°
            selector_xpath: ì…€ë ‰íŠ¸ ë°•ìŠ¤ XPath
            table_container_css: í…Œì´ë¸” ì»¨í…Œì´ë„ˆ CSS ì…€ë ‰í„°
            table_container_xpath: í…Œì´ë¸” ì»¨í…Œì´ë„ˆ XPath
            wait_after_select: ì…€ë ‰í„° ë³€ê²½ í›„ ëŒ€ê¸° ì‹œê°„(ì´ˆ)

        Returns:
            í¬ë¡¤ë§ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        result = {
            'success': False,
            'url': url,
            'crawled_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'data': {},
            'errors': []
        }

        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page()

            try:
                # í˜ì´ì§€ ë¡œë“œ
                await page.goto(url, wait_until='networkidle', timeout=30000)
                await asyncio.sleep(3)  # ì¶”ê°€ ëŒ€ê¸°

                # 1. ì…€ë ‰íŠ¸ ë°•ìŠ¤ ì°¾ê¸°
                select_element = None
                if selector_css:
                    select_element = await page.query_selector(selector_css)
                elif selector_xpath:
                    select_element = await page.query_selector(f'xpath={selector_xpath}')

                if not select_element:
                    result['errors'].append('ì…€ë ‰íŠ¸ ë°•ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')
                    return result

                # 2. ì…€ë ‰í„° ì˜µì…˜ ì¶”ì¶œ
                options = await self.extract_selector_options(page, select_element)
                print(f"âœ“ ì…€ë ‰í„° ì˜µì…˜ {len(options)}ê°œ ë°œê²¬")

                # 3. ê° ì˜µì…˜ë³„ë¡œ í…Œì´ë¸” í¬ë¡¤ë§
                for i, option in enumerate(options):
                    print(f"\nì²˜ë¦¬ ì¤‘ ({i+1}/{len(options)}): {option['text']}")

                    try:
                        # ì˜µì…˜ ì„ íƒ
                        await select_element.select_option(value=option['value'])
                        await asyncio.sleep(wait_after_select)  # í…Œì´ë¸” ë Œë”ë§ ëŒ€ê¸°

                        # í…Œì´ë¸” ì»¨í…Œì´ë„ˆ ì°¾ê¸°
                        container = None
                        if table_container_css:
                            container = await page.query_selector(table_container_css)
                        elif table_container_xpath:
                            container = await page.query_selector(f'xpath={table_container_xpath}')

                        if not container:
                            print(f"  âš ï¸ í…Œì´ë¸” ì»¨í…Œì´ë„ˆë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {option['text']}")
                            continue

                        # í…Œì´ë¸” ì¶”ì¶œ
                        tables = await container.query_selector_all('table')
                        print(f"  âœ“ í…Œì´ë¸” {len(tables)}ê°œ ë°œê²¬")

                        option_data = {
                            'option_value': option['value'],
                            'option_text': option['text'],
                            'tables': []
                        }

                        for j, table in enumerate(tables):
                            table_data = await self.extract_table_data(page, table)
                            if table_data['rows']:  # ë°ì´í„°ê°€ ìˆëŠ” í…Œì´ë¸”ë§Œ
                                option_data['tables'].append({
                                    'table_index': j,
                                    'data': table_data
                                })
                                print(f"    í…Œì´ë¸” {j+1}: {len(table_data['rows'])}í–‰")

                        if option_data['tables']:
                            result['data'][option['value']] = option_data

                    except Exception as e:
                        print(f"  âŒ ì˜µì…˜ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
                        result['errors'].append(f"ì˜µì…˜ '{option['text']}' ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")

                result['success'] = len(result['data']) > 0

            except Exception as e:
                result['errors'].append(f'í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}')

            finally:
                await browser.close()

        return result

    async def crawl_specific_location(self, url: str, xpath: str) -> Dict:
        """
        íŠ¹ì • XPath ìœ„ì¹˜ì˜ í…Œì´ë¸” í¬ë¡¤ë§
        ì˜ˆ: /html/body/div[2]/div/div[2]/div/div[2]/div[4]/div[2]/div[1]/div/div[1]
        """
        result = {
            'success': False,
            'url': url,
            'xpath': xpath,
            'crawled_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'data': None,
            'errors': []
        }

        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page()

            try:
                await page.goto(url, wait_until='networkidle', timeout=30000)
                await asyncio.sleep(3)

                # XPathë¡œ ìš”ì†Œ ì°¾ê¸°
                target = await page.query_selector(f'xpath={xpath}')

                if not target:
                    result['errors'].append(f'XPath ìœ„ì¹˜ì—ì„œ ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {xpath}')
                    return result

                # ìš”ì†Œ ì •ë³´ ì¶”ì¶œ
                tag_name = await target.evaluate('el => el.tagName')
                class_name = await target.get_attribute('class')
                element_id = await target.get_attribute('id')

                result['data'] = {
                    'element_info': {
                        'tag': tag_name,
                        'class': class_name,
                        'id': element_id
                    }
                }

                # ì…€ë ‰íŠ¸ ë°•ìŠ¤ ì°¾ê¸°
                selects = await target.query_selector_all('select')
                if selects:
                    result['data']['select_count'] = len(selects)
                    result['data']['selects'] = []

                    for i, select in enumerate(selects):
                        select_id = await select.get_attribute('id')
                        select_name = await select.get_attribute('name')
                        options = await self.extract_selector_options(page, select)

                        result['data']['selects'].append({
                            'index': i,
                            'id': select_id,
                            'name': select_name,
                            'options': options
                        })

                # í…Œì´ë¸” ì°¾ê¸°
                tables = await target.query_selector_all('table')
                if tables:
                    result['data']['table_count'] = len(tables)
                    result['data']['tables'] = []

                    for i, table in enumerate(tables):
                        table_data = await self.extract_table_data(page, table)
                        result['data']['tables'].append({
                            'index': i,
                            'data': table_data
                        })

                result['success'] = True

            except Exception as e:
                result['errors'].append(f'í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}')

            finally:
                await browser.close()

        return result


async def main():
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    crawler = DynamicTableCrawler()

    # ì˜ˆì œ 1: íŠ¹ì • XPath ìœ„ì¹˜ ë¶„ì„
    print("=" * 60)
    print("íŠ¹ì • ìœ„ì¹˜ í…Œì´ë¸” ë¶„ì„")
    print("=" * 60)

    result = await crawler.crawl_specific_location(
        url="https://www.data.go.kr/data/15001700/openapi.do",
        xpath='/html/body/div[2]/div/div[2]/div/div[2]/div[4]/div[2]/div[1]/div/div[1]'
    )

    print(f"\nì„±ê³µ: {result['success']}")
    if result['success']:
        print(f"ì…€ë ‰íŠ¸ ë°•ìŠ¤: {result['data'].get('select_count', 0)}ê°œ")
        print(f"í…Œì´ë¸”: {result['data'].get('table_count', 0)}ê°œ")

        if result['data'].get('selects'):
            for select_info in result['data']['selects']:
                print(f"\nì…€ë ‰íŠ¸ {select_info['index'] + 1}:")
                print(f"  ID: {select_info['id']}")
                print(f"  ì˜µì…˜: {len(select_info['options'])}ê°œ")

    # ê²°ê³¼ ì €ì¥
    with open('/home/user/python_app/dynamic_table_result.json', 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    print(f"\nğŸ’¾ ê²°ê³¼ê°€ dynamic_table_result.jsonì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    # ì˜ˆì œ 2: ì…€ë ‰í„° ê¸°ë°˜ í¬ë¡¤ë§ (ì‹¤ì œ ì…€ë ‰í„° ì •ë³´ê°€ í•„ìš”)
    # result2 = await crawler.crawl_dynamic_selector_table(
    #     url="https://www.data.go.kr/data/15001700/openapi.do",
    #     selector_css="select#yourSelectId",  # ì‹¤ì œ ì…€ë ‰í„°ë¡œ ë³€ê²½ í•„ìš”
    #     table_container_xpath='/html/body/div[2]/div/div[2]/div/div[2]/div[4]/div[2]/div[1]/div/div[1]'
    # )


if __name__ == "__main__":
    asyncio.run(main())
