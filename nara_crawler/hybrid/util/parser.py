import json
import re
import os
import csv
from datetime import datetime
from xml.etree.ElementTree import Element, SubElement, tostring
from xml.dom import minidom
from functools import lru_cache

class NaraParser:
    """ë‚˜ë¼ì¥í„° API íŒŒì„œ í´ë˜ìŠ¤ - í¬ë¡¤ëŸ¬ í†µí•©ìš©"""
    
    def __init__(self, driver=None):
        self.driver = driver
    
    def extract_api_info(self, swagger_json):
        """API ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ - í¬ë¡¤ëŸ¬ í˜¸í™˜ìš©"""
        if not swagger_json:
            return {}
        
        info = swagger_json.get('info', {})
        api_info = {
            'title': info.get('title', ''),
            'description': info.get('description', ''),
            'version': info.get('version', '')
        }
        
        # í™•ì¥ ì •ë³´
        for key, value in info.items():
            if key.startswith('x-'):
                api_info[key.replace('x-', '')] = value
        
        return api_info

    def extract_base_url(self, swagger_json):
        """Base URL ì¶”ì¶œ - í¬ë¡¤ëŸ¬ í˜¸í™˜ìš©"""
        if not swagger_json:
            return ""
        
        schemes = swagger_json.get('schemes', ['https'])
        host = swagger_json.get('host', '')
        base_path = swagger_json.get('basePath', '')
        
        if host:
            scheme = schemes[0] if schemes else 'https'
            return f"{scheme}://{host}{base_path}"
        return ""

    def extract_endpoints(self, swagger_json):
        """ì—”ë“œí¬ì¸íŠ¸ ì •ë³´ ì¶”ì¶œ - í¬ë¡¤ëŸ¬ í˜¸í™˜ìš©"""
        endpoints = []
        if not swagger_json:
            return endpoints
        
        paths = swagger_json.get('paths', {})
        for path, methods in paths.items():
            for method, data in methods.items():
                if method in ['get', 'post', 'put', 'delete', 'patch']:
                    endpoint = {
                        'method': method.upper(),
                        'path': path,
                        'description': data.get('summary', '') or data.get('description', ''),
                        'parameters': self._extract_swagger_parameters(data.get('parameters', [])),
                        'responses': self._extract_swagger_responses(data.get('responses', {})),
                        'tags': data.get('tags', []),
                        'section': data.get('tags', ['Default'])[0] if data.get('tags') else 'Default'
                    }
                    endpoints.append(endpoint)
        
        return endpoints

    def _extract_swagger_parameters(self, params_list):
        """Swagger íŒŒë¼ë¯¸í„° ì¶”ì¶œ"""
        return [{
            'name': param.get('name', ''),
            'description': param.get('description', ''),
            'required': param.get('required', False),
            'type': param.get('type', '') or (param.get('schema', {}).get('type', '') if 'schema' in param else '')
        } for param in params_list]

    def _extract_swagger_responses(self, responses_dict):
        """Swagger ì‘ë‹µ ì¶”ì¶œ"""
        return [{
            'status_code': status_code,
            'description': data.get('description', '')
        } for status_code, data in responses_dict.items()]


class DataExporter:
    """ë°ì´í„° ë‚´ë³´ë‚´ê¸° í´ë˜ìŠ¤ - í¬ë¡¤ëŸ¬ í†µí•©ìš©"""
    
    @staticmethod
    def save_crawling_result(data, output_dir, api_id, formats=['json', 'xml']):
        """í¬ë¡¤ë§ ê²°ê³¼ ì €ì¥ - ë©”ì¸ ì €ì¥ í•¨ìˆ˜"""
        saved_files, errors = [], []
        
        # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
        table_info = data.get('info', {})
        org_name = table_info.get('ì œê³µê¸°ê´€', 'unknown_org')
        modified_date = table_info.get('ìˆ˜ì •ì¼', 'unknown_date')
        
        # ë¬¸ì„œë²ˆí˜¸ ì¶”ì¶œ
        crawled_url = data.get('crawled_url', '')
        doc_num = 'unknown_doc'
        if crawled_url:
            match = re.search(r'/data/(\d+)/openapi', crawled_url)
            if match:
                doc_num = match.group(1)
        
        # ê¸°ê´€ëª… ì •ì œ
        org_name = re.sub(r'[^\w\s-]', '', org_name)
        org_name = re.sub(r'[\s]+', '_', org_name).strip()

        # data í´ë”ë¥¼ ê¸°ë³¸ ë””ë ‰í† ë¦¬ë¡œ ì‚¬ìš©
        data_dir = './data'

        # API íƒ€ì…ì— ë”°ë¥¸ ë””ë ‰í† ë¦¬ ì„¤ì •
        api_type = data.get('api_type', 'unknown')
        api_category = table_info.get('API ìœ í˜•', '')
        is_link_type = 'LINK' in api_category.upper() if api_category else False

        if api_type == 'link' or is_link_type:
            base_dir = os.path.join(data_dir, 'LINK', org_name)
        elif api_type in ['general', 'general_dynamic']:
            base_dir = os.path.join(data_dir, 'ì¼ë°˜API_old', org_name)
        elif api_type in ['swagger', 'swagger_dynamic']:
            base_dir = os.path.join(data_dir, 'ì¼ë°˜API', org_name)
        else:
            base_dir = os.path.join(data_dir, 'ê¸°íƒ€', org_name)
        
        file_prefix = f"{doc_num}_{modified_date}"
        os.makedirs(base_dir, exist_ok=True)
        
        # í˜•ì‹ë³„ ì €ì¥
        for format_type in formats:
            try:
                if format_type == 'json':
                    file_path = os.path.join(base_dir, f"{file_prefix}.json")
                    success, error = DataExporter._save_as_json(data, file_path)
                    if success:
                        saved_files.append(file_path)
                elif format_type == 'xml':
                    file_path = os.path.join(base_dir, f"{file_prefix}.xml")
                    success, error = DataExporter._save_as_xml(data, file_path)
                    if success:
                        saved_files.append(file_path)
                elif format_type == 'md':
                    file_path = os.path.join(base_dir, f"{file_prefix}.md")
                    success, error = DataExporter._save_as_markdown(data, file_path)
                    if success:
                        saved_files.append(file_path)
                elif format_type == 'csv':
                    # CSVëŠ” data í´ë” ë°”ë¡œ í•˜ìœ„ì— ì €ì¥
                    os.makedirs('./data', exist_ok=True)
                    file_path = os.path.join('./data', 'TOTAL_RESULT_TABLE.CSV')
                    success, error = DataExporter._save_as_csv(data, file_path)
                    if success:
                        saved_files.append(file_path)
                
                if error:
                    errors.append(error)
            except Exception as e:
                errors.append(f"{format_type.upper()} ì €ì¥ ì‹¤íŒ¨: {str(e)}")
        
        return saved_files, errors

    @staticmethod
    def _save_as_json(data, file_path):
        """JSON ì €ì¥"""
        try:
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            return True, None
        except Exception as e:
            return False, f"JSON ì €ì¥ ì‹¤íŒ¨: {str(e)}"

    @staticmethod
    def _save_as_xml(data, file_path):
        """XML ì €ì¥"""
        try:
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            
            def _dict_to_xml(d, parent, name=None):
                if name is None:
                    element = parent
                else:
                    clean_name = re.sub(r'[^a-zA-Z0-9_-]', '_', str(name))
                    if clean_name and clean_name[0].isdigit():
                        clean_name = f"item_{clean_name}"
                    if not clean_name:
                        clean_name = "unnamed_item"
                    element = SubElement(parent, clean_name)
                
                if isinstance(d, dict):
                    for key, value in d.items():
                        _dict_to_xml(value, element, key)
                elif isinstance(d, list):
                    for i, item in enumerate(d):
                        if isinstance(item, dict):
                            _dict_to_xml(item, element, f"item_{i}")
                        else:
                            item_elem = SubElement(element, f"item_{i}")
                            item_elem.text = str(item) if item is not None else ""
                else:
                    element.text = str(d) if d is not None else ""
            
            root = Element("api_documentation")
            _dict_to_xml(data, root)
            
            rough_string = tostring(root, encoding='utf-8')
            reparsed = minidom.parseString(rough_string)
            pretty_xml = reparsed.toprettyxml(indent='  ', encoding='utf-8')
            
            with open(file_path, 'wb') as f:
                f.write(pretty_xml)
            
            return True, None
        except Exception as e:
            return False, f"XML ì €ì¥ ì‹¤íŒ¨: {str(e)}"

    @staticmethod
    def _save_as_markdown(data, file_path):
        """Markdown ì €ì¥"""
        try:
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            md_content = DataExporter._dict_to_markdown(data)
            
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(md_content)
            
            return True, None
        except Exception as e:
            return False, f"Markdown ì €ì¥ ì‹¤íŒ¨: {str(e)}"

    @staticmethod
    def _dict_to_markdown(data):
        """ë”•ì…”ë„ˆë¦¬ë¥¼ Markdownìœ¼ë¡œ ë³€í™˜"""
        api_type = data.get('api_type', 'unknown')

        if api_type in ['swagger', 'swagger_dynamic']:
            return DataExporter._swagger_to_markdown(data)
        elif api_type in ['general', 'general_dynamic']:
            return DataExporter._general_api_to_markdown(data)
        elif api_type == 'link':
            return DataExporter._link_to_markdown(data)
        else:
            return "# API ë¬¸ì„œ\n\nì•Œ ìˆ˜ ì—†ëŠ” API íƒ€ì…ì…ë‹ˆë‹¤."

    @staticmethod
    def _swagger_to_markdown(data):
        """Swagger API Markdown ë³€í™˜"""
        lines = []
        api_info = data.get('api_info', {})
        endpoints = data.get('endpoints', [])
        
        # í—¤ë”
        lines.append(f"# {api_info.get('title', 'API Documentation')}")
        lines.append("")
        if data.get('crawled_time'):
            lines.append(f"**í¬ë¡¤ë§ ì‹œê°„:** {data['crawled_time']}")
        if data.get('crawled_url'):
            lines.append(f"**ì›ë³¸ URL:** {data['crawled_url']}")
        lines.append("")
        
        # API ì •ë³´
        lines.append("## ğŸ“‹ API ì •ë³´")
        lines.append("")
        if api_info.get('description'):
            lines.append(f"**ì„¤ëª…:** {api_info['description']}")
            lines.append("")
        if api_info.get('base_url'):
            lines.append(f"**Base URL:** `{api_info['base_url']}`")
            lines.append("")
        
        # ì—”ë“œí¬ì¸íŠ¸
        if endpoints:
            base_url = api_info.get('base_url', '')
            lines.append(f"## ğŸ”— API ì—”ë“œí¬ì¸íŠ¸ ({len(endpoints)}ê°œ)")
            lines.append("")
            
            if base_url:
                lines.append(f"**Base URL:** `{base_url}`")
                lines.append("")
            
            for endpoint in endpoints:
                method = endpoint.get('method', 'GET')
                path = endpoint.get('path', '')
                description = endpoint.get('description', '')
                full_url = f"{base_url}{path}" if base_url and path else path
                
                lines.append(f"#### `{method}` {path}")
                if base_url:
                    lines.append(f"**ì™„ì „í•œ URL:** `{full_url}`")
                lines.append("")
                if description:
                    lines.append(f"**ì„¤ëª…:** {description}")
                lines.append("")
                
                # íŒŒë¼ë¯¸í„°
                parameters = endpoint.get('parameters', [])
                if parameters:
                    lines.append("**íŒŒë¼ë¯¸í„°:**")
                    lines.append("")
                    lines.append("| ì´ë¦„ | íƒ€ì… | í•„ìˆ˜ | ì„¤ëª… |")
                    lines.append("|------|------|------|------|")
                    for param in parameters:
                        name = str(param.get('name', '')).replace('|', '\\|')
                        param_type = str(param.get('type', '')).replace('|', '\\|')
                        required = "âœ…" if param.get('required', False) else "âŒ"
                        desc = str(param.get('description', '')).replace('|', '\\|')
                        if len(desc) > 50:
                            desc = desc[:50] + "..."
                        lines.append(f"| `{name}` | {param_type} | {required} | {desc} |")
                    lines.append("")
                
                # ì‘ë‹µ
                responses = endpoint.get('responses', [])
                if responses:
                    lines.append("**ì‘ë‹µ:**")
                    lines.append("")
                    lines.append("| ìƒíƒœ ì½”ë“œ | ì„¤ëª… |")
                    lines.append("|-----------|------|")
                    for response in responses:
                        status_code = str(response.get('status_code', '')).replace('|', '\\|')
                        desc = str(response.get('description', '')).replace('|', '\\|')
                        if len(desc) > 80:
                            desc = desc[:80] + "..."
                        lines.append(f"| `{status_code}` | {desc} |")
                    lines.append("")
                
                lines.append("---")
                lines.append("")
        
        # í‘¸í„°
        lines.append("## ğŸ“ ìƒì„± ì •ë³´")
        lines.append("")
        lines.append("ì´ ë¬¸ì„œëŠ” ë‚˜ë¼ì¥í„° API í¬ë¡¤ëŸ¬ì— ì˜í•´ ìë™ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
        if data.get('api_id'):
            lines.append(f"**API ID:** {data['api_id']}")
        if api_info.get('base_url'):
            lines.append(f"**Base URL:** {api_info['base_url']}")
        
        return "\n".join(lines)

    @staticmethod
    def _general_api_to_markdown(data):
        """ì¼ë°˜ API Markdown ë³€í™˜"""
        lines = []
        general_info = data.get('general_api_info', {})
        detail_info = general_info.get('detail_info', {})
        
        # í—¤ë”
        title = detail_info.get('description', 'API Documentation')
        if len(title) > 50:
            title = title[:50] + "..."
        lines.append(f"# {title}")
        lines.append("")
        if data.get('crawled_time'):
            lines.append(f"**í¬ë¡¤ë§ ì‹œê°„:** {data['crawled_time']}")
        if data.get('crawled_url'):
            lines.append(f"**ì›ë³¸ URL:** {data['crawled_url']}")
        lines.append("")
        
        # ìƒì„¸ì •ë³´
        if detail_info:
            lines.append("## ğŸ“‹ API ìƒì„¸ì •ë³´")
            lines.append("")
            if detail_info.get('description'):
                lines.append(f"**ê¸°ëŠ¥ ì„¤ëª…:** {detail_info['description']}")
                lines.append("")
            if detail_info.get('request_url'):
                lines.append(f"**ìš”ì²­ ì£¼ì†Œ:** `{detail_info['request_url']}`")
                lines.append("")
            if detail_info.get('service_url'):
                lines.append(f"**ì„œë¹„ìŠ¤ URL:** `{detail_info['service_url']}`")
                lines.append("")
        
        # í‘¸í„°
        lines.append("## ğŸ“ ìƒì„± ì •ë³´")
        lines.append("")
        lines.append("ì´ ë¬¸ì„œëŠ” ë‚˜ë¼ì¥í„° API í¬ë¡¤ëŸ¬ì— ì˜í•´ ìë™ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
        lines.append("**API íƒ€ì…:** ì¼ë°˜ API (Swagger ë¯¸ì§€ì›)")
        if data.get('api_id'):
            lines.append(f"**API ID:** {data['api_id']}")
        
        return "\n".join(lines)

    @staticmethod
    def _link_to_markdown(data):
        """LINK íƒ€ì… API Markdown ë³€í™˜"""
        lines = []
        table_info = data.get('info', {})
        
        lines.append("# LINK íƒ€ì… API")
        lines.append("")
        if data.get('crawled_time'):
            lines.append(f"**í¬ë¡¤ë§ ì‹œê°„:** {data['crawled_time']}")
        if data.get('crawled_url'):
            lines.append(f"**ì›ë³¸ URL:** {data['crawled_url']}")
        lines.append("")
        
        lines.append("## ğŸ“‹ API ì •ë³´")
        lines.append("")
        lines.append("ì´ APIëŠ” LINK íƒ€ì…ìœ¼ë¡œ, ì™¸ë¶€ ë§í¬ë¥¼ í†µí•´ ì œê³µë©ë‹ˆë‹¤.")
        lines.append("")
        
        if table_info:
            lines.append("## ğŸ“Š ìƒì„¸ ì •ë³´")
            lines.append("")
            for key, value in table_info.items():
                lines.append(f"**{key}:** {value}")
            lines.append("")
        
        lines.append("## ğŸ“ ìƒì„± ì •ë³´")
        lines.append("")
        lines.append("ì´ ë¬¸ì„œëŠ” ë‚˜ë¼ì¥í„° API í¬ë¡¤ëŸ¬ì— ì˜í•´ ìë™ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
        lines.append("**API íƒ€ì…:** LINK (ì™¸ë¶€ ë§í¬ ì œê³µ)")
        if data.get('api_id'):
            lines.append(f"**API ID:** {data['api_id']}")
        
        return "\n".join(lines)

    @staticmethod
    def _save_as_csv(data, file_path):
        """CSV ì €ì¥ - ëª¨ë“  ë¬¸ì„œ ì •ë³´ ëˆ„ì """
        try:
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            info_data = data.get('info', {})
            
            if not info_data:
                return False, "ì €ì¥í•  í…Œì´ë¸” ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."
            
            target_fields = [
                'ë¶„ë¥˜ì²´ê³„', 'ì œê³µê¸°ê´€', 'ê´€ë¦¬ë¶€ì„œëª…', 'ê´€ë¦¬ë¶€ì„œ ì „í™”ë²ˆí˜¸', 'API ìœ í˜•',
                'ë°ì´í„°í¬ë§·', 'í™œìš©ì‹ ì²­', 'í‚¤ì›Œë“œ', 'ë“±ë¡ì¼', 'ìˆ˜ì •ì¼', 'ë¹„ìš©ë¶€ê³¼ìœ ë¬´', 'ì´ìš©í—ˆë½ë²”ìœ„'
            ]
            
            filtered_data = {
                'ë¬¸ì„œë²ˆí˜¸': data.get('api_id', ''),
                'í¬ë¡¤ë§ì‹œê°„': data.get('crawled_time', ''),
                'URL': data.get('crawled_url', '')
            }
            
            for field in target_fields:
                filtered_data[field] = info_data.get(field, '')
            
            file_exists = os.path.isfile(file_path)
            
            with open(file_path, 'a', encoding='cp949', newline='') as f:
                writer = csv.DictWriter(f, fieldnames=filtered_data.keys())
                if not file_exists:
                    writer.writeheader()
                writer.writerow(filtered_data)
            
            return True, None
        except Exception as e:
            return False, f"CSV ì €ì¥ ì‹¤íŒ¨: {str(e)}"