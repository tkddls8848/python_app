##ì£¼ê¸°ì„± ê³¼ê±° ë°ì´í„° ì¶”ì¶œ ì‘ì—… í•„ìš”
##URL Linkë¡œ ë°ì´í„° ì£¼ëŠ” ì¼€ì´ìŠ¤ ì‘ì—… í•„ìš”

import requests
from bs4 import BeautifulSoup
import json
import os
import argparse
from datetime import datetime
import time
from metadata_fileData import FileDataMetadataScanner
import pandas as pd
import glob


class FileDataCSVCrawler:
    """data.go.kr íŒŒì¼ë°ì´í„° CSV ë‹¤ìš´ë¡œë“œ ë° ë©”íƒ€ë°ì´í„° ì €ì¥"""
    
    def __init__(self, timeout=30):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        self.timeout = timeout
    
    def save_metadata(self, doc_num, metadata):
        """ë©”íƒ€ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        try:
            save_dir = "/data/download_fileData"
            os.makedirs(save_dir, exist_ok=True)
            metadata_filename = f"{doc_num}_metadata.json"
            metadata_save_path = os.path.join(save_dir, metadata_filename)
            
            metadata_data = {
                'doc_num': doc_num,
                'url': f"https://www.data.go.kr/data/{doc_num}/fileData.do",
                'metadata': metadata,
                'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'item_count': len(metadata) if isinstance(metadata, dict) else 0
            }
            
            with open(metadata_save_path, 'w', encoding='utf-8') as f:
                json.dump(metadata_data, f, ensure_ascii=False, indent=2)
            
            print(f"ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ: {metadata_save_path} ({len(metadata)}ê°œ í•­ëª©)")
            return metadata_save_path
            
        except Exception as e:
            print(f"ë©”íƒ€ë°ì´í„° ì €ì¥ ì‹¤íŒ¨ ({doc_num}): {str(e)}")
            return None
    
    def download_csv_and_save_metadata(self, doc_num, metadata):
        """íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë° ë©”íƒ€ë°ì´í„° ì €ì¥"""
        url = f"https://www.data.go.kr/data/{doc_num}/fileData.do"
        print(f"ë¬¸ì„œë²ˆí˜¸ {doc_num} ì²˜ë¦¬ ì¤‘: {url}")
        
        result = {
            'doc_num': doc_num,
            'url': url,
            'file_downloaded': False,
            'metadata_saved': False,
            'file_path': None,
            'metadata_file': None,
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'error': None
        }
        
        try:
            # íŒŒì¼ ë‹¤ìš´ë¡œë“œ
            response = self.session.get(url)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, "html.parser")
            scripts = soup.find_all("script", type="application/ld+json")
            
            file_downloaded = False
            for script in scripts:
                try:
                    data = json.loads(script.string)
                    if "distribution" in data and isinstance(data["distribution"], list):
                        for dist in data["distribution"]:
                            if "contentUrl" in dist:
                                file_url = dist["contentUrl"]
                                encoding_format = dist.get("encodingFormat", "").upper()
                                
                                print(f"íŒŒì¼ ë‹¤ìš´ë¡œë“œ URL: {file_url}")
                                print(f"íŒŒì¼ í˜•ì‹: {encoding_format}")
                                
                                # contentUrl íŒ¨í„´ ë¶„ì„
                                if "data.go.kr" in file_url:
                                    # data.go.kr ë„ë©”ì¸ì¸ ê²½ìš° - ê¸°ì¡´ ë¡œì§ëŒ€ë¡œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
                                    file_extension = self.get_extension_from_format(encoding_format)
                                    print(f"data.go.kr ë„ë©”ì¸ ê°ì§€ -> íŒŒì¼ ë‹¤ìš´ë¡œë“œ: {file_extension}")
                                    
                                    # contentUrlì—ì„œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
                                    file_response = self.session.get(file_url)
                                    file_response.raise_for_status()
                                    
                                    save_dir = "/data/download_fileData"
                                    os.makedirs(save_dir, exist_ok=True)
                                    filename = f"file_{doc_num}{file_extension}"
                                    save_path = os.path.join(save_dir, filename)
                                    
                                    with open(save_path, "wb") as f:
                                        f.write(file_response.content)
                                    
                                    print(f"íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {save_path}")
                                    result['file_downloaded'] = True
                                    result['file_path'] = save_path
                                    result['file_extension'] = file_extension
                                    result['encoding_format'] = encoding_format
                                    result['download_type'] = 'file_download'
                                    
                                else:
                                    # ë‹¤ë¥¸ ê¸°ê´€ ì£¼ì†Œì¸ ê²½ìš° - URLì„ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥
                                    print(f"ì™¸ë¶€ ê¸°ê´€ ì£¼ì†Œ ê°ì§€ -> URL í…ìŠ¤íŠ¸ ì €ì¥")
                                    
                                    save_dir = "/data/download_fileData"
                                    os.makedirs(save_dir, exist_ok=True)
                                    filename = f"file_{doc_num}.txt"
                                    save_path = os.path.join(save_dir, filename)
                                    
                                    # URL ì •ë³´ë¥¼ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥
                                    url_info = f"""ë¬¸ì„œë²ˆí˜¸: {doc_num}
ì›ë³¸ URL: {url}
íŒŒì¼ ë‹¤ìš´ë¡œë“œ URL: {file_url}
íŒŒì¼ í˜•ì‹: {encoding_format}
ì €ì¥ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë§í¬:
{file_url}

ì°¸ê³ : ì´ íŒŒì¼ì€ ì™¸ë¶€ ê¸°ê´€ì—ì„œ ì œê³µí•˜ëŠ” ìë£Œë¡œ, ì§ì ‘ ë‹¤ìš´ë¡œë“œê°€ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.
ìœ„ URLì„ ë¸Œë¼ìš°ì €ì—ì„œ ì—´ì–´ ìˆ˜ë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.
"""
                                    
                                    with open(save_path, "w", encoding="utf-8") as f:
                                        f.write(url_info)
                                    
                                    print(f"URL ì •ë³´ ì €ì¥ ì™„ë£Œ: {save_path}")
                                    result['file_downloaded'] = True
                                    result['file_path'] = save_path
                                    result['file_extension'] = '.txt'
                                    result['encoding_format'] = encoding_format
                                    result['download_type'] = 'url_info'
                                
                                file_downloaded = True
                                break
                        
                        if file_downloaded:
                            break
                            
                except Exception as e:
                    print(f"ë¬¸ì„œë²ˆí˜¸ {doc_num} ë‚´ ìŠ¤í¬ë¦½íŠ¸ ì²˜ë¦¬ ì—ëŸ¬: {e}")
                    continue
            
            # ë©”íƒ€ë°ì´í„° ì €ì¥
            if metadata:
                metadata_file = self.save_metadata(doc_num, metadata)
                if metadata_file:
                    result['metadata_saved'] = True
                    result['metadata_file'] = metadata_file
                    result['metadata_item_count'] = len(metadata) if isinstance(metadata, dict) else 0
                else:
                    print(f"ë©”íƒ€ë°ì´í„° ì €ì¥ ì‹¤íŒ¨ ({doc_num})")
            
        except Exception as e:
            error_msg = f"ë¬¸ì„œë²ˆí˜¸ {doc_num} ì²˜ë¦¬ ì‹¤íŒ¨: {e}"
            print(error_msg)
            result['error'] = str(e)
        
        return result
    
    def get_extension_from_format(self, encoding_format):
        """encodingFormatì„ ê¸°ë°˜ìœ¼ë¡œ íŒŒì¼ í™•ì¥ìë¥¼ ë°˜í™˜"""
        format_to_ext = {
            'CSV': '.csv',
            'XLS': '.xls',
            'XLSX': '.xlsx',
            'PPT': '.ppt',
            'PPTX': '.pptx',
            'DOC': '.doc',
            'DOCX': '.docx',
            'PDF': '.pdf',
            'ZIP': '.zip',
            'TXT': '.txt',
            'XML': '.xml',
            'JSON': '.json',
            'HWP': '.hwp',
            'JPG': '.jpg',
            'JPEG': '.jpeg',
            'PNG': '.png',
            'GIF': '.gif',
            'BMP': '.bmp',
            'TIFF': '.tiff',
            'AVI': '.avi',
            'MP4': '.mp4',
            'MP3': '.mp3',
            'WAV': '.wav',
            'RAR': '.rar',
            '7Z': '.7z',
            'TAR': '.tar',
            'GZ': '.gz'
        }
        
        return format_to_ext.get(encoding_format, '.bin')
    
    def close(self):
        """Close session"""
        self.session.close()
    
    def convert_metadata_to_csv(self, metadata_dir="/data/download_fileData"):
        """ë©”íƒ€ë°ì´í„° JSON íŒŒì¼ë“¤ì„ êµ¬ì¡°í™”ëœ CSVë¡œ ë³€í™˜"""
        try:
            # JSON íŒŒì¼ë“¤ ì°¾ê¸°
            json_pattern = os.path.join(metadata_dir, "*_metadata.json")
            json_files = glob.glob(json_pattern)
            
            if not json_files:
                print("âŒ ë³€í™˜í•  ë©”íƒ€ë°ì´í„° JSON íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                return None
            
            print(f"\nğŸ”„ ë©”íƒ€ë°ì´í„° JSON â†’ CSV ë³€í™˜ ì‹œì‘: {len(json_files)}ê°œ íŒŒì¼")
            
            # ë©”íƒ€ë°ì´í„° ë°ì´í„° ìˆ˜ì§‘
            metadata_list = []
            
            for json_file in json_files:
                try:
                    with open(json_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
                    doc_num = data.get('doc_num', '')
                    url = data.get('url', '')
                    timestamp = data.get('timestamp', '')
                    item_count = data.get('item_count', 0)
                    
                    # ë©”íƒ€ë°ì´í„° ì •ë³´ ì¶”ì¶œ
                    metadata = data.get('metadata', {})
                    
                    # í‰ë©´í™”ëœ ë°ì´í„° êµ¬ì¡° ìƒì„±
                    flat_data = {
                        'doc_num': doc_num,
                        'url': url,
                        'timestamp': timestamp,
                        'item_count': item_count,
                        'name': metadata.get('name', ''),
                        'alternateName': metadata.get('alternateName', ''),
                        'description': metadata.get('description', ''),
                        'keywords': metadata.get('keywords', ''),
                        'license': metadata.get('license', ''),
                        'dateCreated': metadata.get('dateCreated', ''),
                        'dateModified': metadata.get('dateModified', ''),
                        'datePublished': metadata.get('datePublished', ''),
                        'additionalType': metadata.get('additionalType', ''),
                        'datasetTimeInterval': metadata.get('datasetTimeInterval', ''),
                        'encodingFormat': metadata.get('encodingFormat', ''),
                        'legislation': metadata.get('legislation', ''),
                        'creator_name': metadata.get('creator', {}).get('name', ''),
                        'creator_contactType': metadata.get('creator', {}).get('contactPoint', {}).get('contactType', ''),
                        'creator_telephone': metadata.get('creator', {}).get('contactPoint', {}).get('telephone', ''),
                        '@context': metadata.get('@context', ''),
                        '@type': metadata.get('@type', '')
                    }
                    
                    metadata_list.append(flat_data)
                    
                except Exception as e:
                    print(f"âš ï¸ JSON íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨ ({json_file}): {str(e)}")
                    continue
            
            if not metadata_list:
                print("âŒ ë³€í™˜í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return None
            
            # DataFrame ìƒì„±
            df = pd.DataFrame(metadata_list)
            
            # CSV íŒŒì¼ë¡œ ì €ì¥
            csv_filename = "metadata_summary.csv"
            csv_path = os.path.join(metadata_dir, csv_filename)
            
            df.to_csv(csv_path, index=False, encoding='utf-8-sig')
            
            print(f"âœ… ë©”íƒ€ë°ì´í„° CSV ë³€í™˜ ì™„ë£Œ: {csv_path}")
            print(f"   ğŸ“Š ì´ {len(metadata_list)}ê°œ ë ˆì½”ë“œ")
            print(f"   ğŸ“‹ ì»¬ëŸ¼ ìˆ˜: {len(df.columns)}ê°œ")
            
            # ì»¬ëŸ¼ ì •ë³´ ì¶œë ¥
            print(f"\nğŸ“‹ CSV ì»¬ëŸ¼ ì •ë³´:")
            for i, col in enumerate(df.columns, 1):
                print(f"   {i:2d}. {col}")
            
            return csv_path
            
        except Exception as e:
            print(f"âŒ ë©”íƒ€ë°ì´í„° CSV ë³€í™˜ ì‹¤íŒ¨: {str(e)}")
            return None


def main():
    """Main function"""
    parser = argparse.ArgumentParser(description="data.go.kr ë¬¸ì„œë²ˆí˜¸ ë²”ìœ„ë³„ íŒŒì¼ë°ì´í„° csv ë‹¤ìš´ë¡œë“œ ë° ë©”íƒ€ë°ì´í„° ì €ì¥")
    parser.add_argument('-s', '--start', type=int, required=True, help='ì‹œì‘ ë¬¸ì„œë²ˆí˜¸')
    parser.add_argument('-e', '--end', type=int, required=True, help='ë ë¬¸ì„œë²ˆí˜¸')
    parser.add_argument('--timeout', type=int, default=30, help='Request timeout in seconds (default: 30)')
    
    args = parser.parse_args()
    
    # Input validation
    if args.start > args.end:
        print("ERROR: Start number cannot be greater than end number.")
        return
    
    # ê²°ê³¼ ì €ì¥ìš©
    results = {
        'total_scanned': args.end - args.start + 1,
        'valid_numbers': 0,
        'file_success': 0,
        'file_failed': 0,
        'metadata_success': 0,
        'metadata_failed': 0,
        'start_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'details': []
    }
    
    crawler = FileDataCSVCrawler(timeout=args.timeout)
    
    try:
        # ë©”íƒ€ë°ì´í„° ìŠ¤ìº”ì„ í†µí•´ ìœ íš¨í•œ ë¬¸ì„œë²ˆí˜¸ í™•ì¸
        scanner = FileDataMetadataScanner(args.start, args.end)
        scanner.scan_range()
        
        # ê²°ê³¼ ì €ì¥
        scanner.save_results()
        
        # ìš”ì•½ ì¶œë ¥
        scanner.print_summary()
        
        # íŒŒì¼ ë°ì´í„°ê°€ ìˆëŠ” ë²ˆí˜¸ë“¤ ë°˜í™˜
        valid_numbers = scanner.results['file_numbers']
        
        print(f"\nâœ… ë©”íƒ€ë°ì´í„° ìŠ¤ìº” ì™„ë£Œ!")
        print(f"   ğŸ“‹ ì „ì²´ ìŠ¤ìº”: {scanner.results['total']}ê°œ")
        print(f"   âœ… ìœ íš¨í•œ ë²ˆí˜¸: {len(valid_numbers)}ê°œ")
        print(f"   ğŸ“Š í•„í„°ë§ ë¹„ìœ¨: {(len(valid_numbers) / scanner.results['total'] * 100):.1f}%")
        
        results['valid_numbers'] = len(valid_numbers)
        
        if not valid_numbers:
            print("âŒ ìœ íš¨í•œ ë²ˆí˜¸ê°€ ì—†ìŠµë‹ˆë‹¤. ì²˜ë¦¬ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            return
        
        print(f"\nğŸ“‹ í¬ë¡¤ë§ ì‹œì‘: {len(valid_numbers)}ê°œ ìœ íš¨í•œ ë¬¸ì„œë²ˆí˜¸")
        
        for doc_num in valid_numbers:
            # í•´ë‹¹ ë²ˆí˜¸ì˜ ë©”íƒ€ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            metadata = scanner.results['details'].get(doc_num, {}).get('metadata', {})
            result = crawler.download_csv_and_save_metadata(doc_num, metadata)
            results['details'].append(result)
            
            if result['file_downloaded']:
                results['file_success'] += 1
            else:
                results['file_failed'] += 1
            
            if result['metadata_saved']:
                results['metadata_success'] += 1
            else:
                results['metadata_failed'] += 1
            
            # ì„œë²„ ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ëŒ€ê¸°
            time.sleep(0.5)
    
    finally:
        crawler.close()
    
    # ê²°ê³¼ ìš”ì•½ ì €ì¥
    results['end_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    results['metadata_filter_rate'] = f"{(results['valid_numbers'] / results['total_scanned'] * 100):.1f}%" if results['total_scanned'] > 0 else "0%"
    results['file_success_rate'] = f"{(results['file_success'] / results['valid_numbers'] * 100):.1f}%" if results['valid_numbers'] > 0 else "0%"
    results['metadata_success_rate'] = f"{(results['metadata_success'] / results['valid_numbers'] * 100):.1f}%" if results['valid_numbers'] > 0 else "0%"
    
    # ê²°ê³¼ ì €ì¥
    save_dir = "/data/download_fileData"
    os.makedirs(save_dir, exist_ok=True)
    summary_file = os.path.join(save_dir, "processing_summary.json")
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    # ìµœì¢… ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 50)
    print("íŒŒì¼ë°ì´í„° ë‹¤ìš´ë¡œë“œ ë° ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ!")
    print("=" * 50)
    print(f"ì „ì²´ ìŠ¤ìº”: {results['total_scanned']}ê°œ")
    print(f"ìœ íš¨í•œ ë²ˆí˜¸: {results['valid_numbers']}ê°œ ({results['metadata_filter_rate']})")
    print(f"íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì„±ê³µ: {results['file_success']}ê°œ ({results['file_success_rate']})")
    print(f"íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {results['file_failed']}ê°œ")
    print(f"ë©”íƒ€ë°ì´í„° ì €ì¥ ì„±ê³µ: {results['metadata_success']}ê°œ ({results['metadata_success_rate']})")
    print(f"ë©”íƒ€ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {results['metadata_failed']}ê°œ")
    print(f"ê²°ê³¼ ìœ„ì¹˜: {save_dir}")
    print(f"ìš”ì•½ íŒŒì¼: processing_summary.json")
    
    # ë©”íƒ€ë°ì´í„° JSONì„ CSVë¡œ ë³€í™˜
    if results['metadata_success'] > 0:
        csv_path = crawler.convert_metadata_to_csv(save_dir)
        if csv_path:
            print(f"ğŸ“Š ë©”íƒ€ë°ì´í„° ìš”ì•½ CSV: {os.path.basename(csv_path)}")


if __name__ == '__main__':
    main()