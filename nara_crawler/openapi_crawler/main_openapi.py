"""
ë©”ì¸ í¬ë¡¤ëŸ¬ ì œì–´ íŒŒì¼
BeautifulSoupê³¼ Playwrightë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì¡°í•©í•˜ì—¬ ì‚¬ìš©

ë©”íƒ€ë°ì´í„° ìŠ¤ìº”ì€ util/scanner/base_scanner.py ê¸°ë°˜ìœ¼ë¡œ ì²˜ë¦¬:
- util/scanner/metadata_fileData.py: FileData ìŠ¤ìº”
- util/scanner/metadata_openapi.py: OpenAPI ìŠ¤ìº”
- util/scanner/metadata_standard.py: Standard ìŠ¤ìº”
"""

import asyncio
import argparse
import os
import json
from datetime import datetime
from typing import List, Dict, Tuple
import time

from bs_crawler import BSCrawler
from playwright_crawler import PlaywrightCrawler
from util.parser import DataExporter
from util.scanner.metadata_openapi import OpenAPIMetadataScanner

class HybridCrawler:
    def __init__(self, output_dir: str, formats: List[str], max_workers: int = 40):
        self.output_dir = output_dir
        self.formats = formats
        self.max_workers = max_workers
        
        # BSëŠ” ë” ë§ì€ ë™ì‹œ ì‘ì—… ê°€ëŠ¥
        self.bs_crawler = BSCrawler(max_workers=max_workers * 2)
        # PlaywrightëŠ” ë¦¬ì†ŒìŠ¤ ì œí•œ
        self.pw_crawler = PlaywrightCrawler(max_workers=max(max_workers // 2, 5))
        
        # í†µê³„ ì •ë³´
        self.stats = {
            'bs_success': 0,
            'bs_failed': 0,
            'pw_success': 0,
            'pw_failed': 0,
            'total_time': 0,
            'url_timings': {}
        }
    
    def save_results(self, results: List[Dict]) -> Dict:
        """ê²°ê³¼ ì €ì¥"""
        saved_info = {
            'total_saved': 0,
            'failed_saves': 0,
            'saved_files': []
        }
        
        os.makedirs(self.output_dir, exist_ok=True)
        
        for result in results:
            if result.get('success') and result.get('data'):
                api_id = result.get('api_id', 'unknown')
                saved_files, save_errors = DataExporter.save_crawling_result(
                    result['data'], 
                    self.output_dir, 
                    api_id, 
                    self.formats
                )
                
                if saved_files:
                    saved_info['total_saved'] += 1
                    saved_info['saved_files'].extend(saved_files)
                else:
                    saved_info['failed_saves'] += 1
        
        return saved_info
    
    async def crawl_with_fallback(self, urls: List[str]) -> List[Dict]:
        """
        BeautifulSoup ìš°ì„  ì‹œë„, ì‹¤íŒ¨ì‹œ Playwrightë¡œ fallback
        """
        print(f"\nğŸ“Š í¬ë¡¤ë§ ì‹œì‘: ì´ {len(urls)}ê°œ URL")
        print(f"   - BeautifulSoup ìš°ì„  ì‹œë„ (ë¹ ë¥¸ ì²˜ë¦¬)")
        print(f"   - í•„ìš”ì‹œ Playwright fallback (ë™ì  ì½˜í…ì¸ )")
        
        all_results = []
        start_time = time.time()
        
        # 1ë‹¨ê³„: BeautifulSoupìœ¼ë¡œ ì‹œë„
        print("\nğŸš€ 1ë‹¨ê³„: BeautifulSoup í¬ë¡¤ë§...")
        bs_results, failed_urls = await self.bs_crawler.crawl_batch(urls)
        
        # BS ê²°ê³¼ ì²˜ë¦¬
        for result in bs_results:
            all_results.append(result)
            self.stats['bs_success'] += 1
            self.stats['url_timings'][result['url']] = {
                'method': 'beautifulsoup',
                'success': True
            }
        
        print(f"   âœ… BeautifulSoup ì„±ê³µ: {len(bs_results)}ê°œ")
        print(f"   âš ï¸  BeautifulSoup ì‹¤íŒ¨: {len(failed_urls)}ê°œ")
        
        # 2ë‹¨ê³„: ì‹¤íŒ¨í•œ URLì„ Playwrightë¡œ ì¬ì‹œë„
        if failed_urls:
            print(f"\nğŸ”„ 2ë‹¨ê³„: Playwrightë¡œ {len(failed_urls)}ê°œ ì¬ì‹œë„...")
            pw_results = await self.pw_crawler.crawl_batch(failed_urls)
            
            for result in pw_results:
                all_results.append(result)
                if result['success']:
                    self.stats['pw_success'] += 1
                else:
                    self.stats['pw_failed'] += 1
                
                self.stats['url_timings'][result['url']] = {
                    'method': 'playwright',
                    'success': result['success']
                }
            
            print(f"   âœ… Playwright ì„±ê³µ: {self.stats['pw_success']}ê°œ")
            print(f"   âŒ Playwright ì‹¤íŒ¨: {self.stats['pw_failed']}ê°œ")
        
        self.stats['total_time'] = time.time() - start_time
        
        return all_results
    

    
    async def smart_crawl(self, urls: List[str]) -> List[Dict]:
        """
        URL íŒ¨í„´ ë¶„ì„ìœ¼ë¡œ ìŠ¤ë§ˆíŠ¸ í¬ë¡¤ë§
        ë™ì  ì½˜í…ì¸ ê°€ ì˜ˆìƒë˜ëŠ” URLì€ ë°”ë¡œ Playwright ì‚¬ìš©
        """
        # URL íŒ¨í„´ìœ¼ë¡œ ë¶„ë¥˜
        static_urls = []
        dynamic_urls = []
        
        # ë™ì  ì½˜í…ì¸  íŒíŠ¸ íŒ¨í„´
        dynamic_patterns = [
            'swagger-ui',
            'api-docs',
            'interactive',
            'dynamic',
            '/v2/api',
            '/v3/api'
        ]
        
        for url in urls:
            # URL íŒ¨í„´ ì²´í¬
            is_dynamic = any(pattern in url.lower() for pattern in dynamic_patterns)
            
            if is_dynamic:
                dynamic_urls.append(url)
            else:
                static_urls.append(url)
        
        print(f"\nğŸ“Š ìŠ¤ë§ˆíŠ¸ í¬ë¡¤ë§ ë¶„ì„:")
        print(f"   - ì •ì  ì˜ˆìƒ (BS): {len(static_urls)}ê°œ")
        print(f"   - ë™ì  ì˜ˆìƒ (PW): {len(dynamic_urls)}ê°œ")
        
        all_results = []
        
        # BeautifulSoup ë°°ì¹˜
        if static_urls:
            print("\nğŸš€ ì •ì  ì½˜í…ì¸  í¬ë¡¤ë§ (BeautifulSoup)...")
            bs_results, failed_urls = await self.bs_crawler.crawl_batch(static_urls)
            all_results.extend(bs_results)
            
            # ì‹¤íŒ¨í•œ ê²ƒì€ dynamic_urlsì— ì¶”ê°€
            dynamic_urls.extend(failed_urls)
            
            for result in bs_results:
                self.stats['bs_success'] += 1
        
        # Playwright ë°°ì¹˜
        if dynamic_urls:
            print(f"\nğŸ”„ ë™ì  ì½˜í…ì¸  í¬ë¡¤ë§ (Playwright): {len(dynamic_urls)}ê°œ...")
            pw_results = await self.pw_crawler.crawl_batch(dynamic_urls)
            all_results.extend(pw_results)

            for result in pw_results:
                if result['success']:
                    self.stats['pw_success'] += 1
                else:
                    self.stats['pw_failed'] += 1

        return all_results

    async def optimized_crawl(self, urls: List[str]) -> List[Dict]:
        """
        ìµœì í™”ëœ í¬ë¡¤ë§: LINKëŠ” ì •ì , ë‚˜ë¨¸ì§€ëŠ” ë™ì 
        1. ëª¨ë“  URLì„ ë¹ ë¥´ê²Œ ìŠ¤ìº”í•˜ì—¬ LINK íƒ€ì… ë¶„ë¥˜
        2. LINK íƒ€ì… â†’ BeautifulSoupìœ¼ë¡œ í¬ë¡¤ë§
        3. ë‚˜ë¨¸ì§€(Swagger, General) â†’ Playwrightë¡œ í¬ë¡¤ë§
        """
        print(f"\nğŸ“Š í¬ë¡¤ë§ ì‹œì‘: ì´ {len(urls)}ê°œ URL")
        print(f"   - 1ë‹¨ê³„: LINK íƒ€ì… ë¶„ë¥˜")
        print(f"   - 2ë‹¨ê³„: LINK â†’ ì •ì  í¬ë¡¤ë§ (BS)")
        print(f"   - 3ë‹¨ê³„: Swagger/General â†’ ë™ì  í¬ë¡¤ë§ (PW)")

        all_results = []
        start_time = time.time()

        # 1ë‹¨ê³„: LINK íƒ€ì… ë¶„ë¥˜
        print("\nğŸ” 1ë‹¨ê³„: URL íƒ€ì… ë¶„ë¥˜ ì¤‘...")
        link_urls, other_urls = await self.bs_crawler.classify_urls_by_type(urls)

        print(f"   - LINK íƒ€ì…: {len(link_urls)}ê°œ")
        print(f"   - Swagger/General: {len(other_urls)}ê°œ")

        # 2ë‹¨ê³„: LINK íƒ€ì…ì€ BeautifulSoupìœ¼ë¡œ í¬ë¡¤ë§
        if link_urls:
            print(f"\nğŸš€ 2ë‹¨ê³„: LINK íƒ€ì… í¬ë¡¤ë§ (BeautifulSoup): {len(link_urls)}ê°œ...")
            bs_results, failed_urls = await self.bs_crawler.crawl_batch(link_urls)
            all_results.extend(bs_results)

            for result in bs_results:
                self.stats['bs_success'] += 1

            # LINKì¸ë° ì‹¤íŒ¨í•œ ê²ƒë„ ë™ì ìœ¼ë¡œ ì¬ì‹œë„
            if failed_urls:
                print(f"   âš ï¸  LINK íƒ€ì… ì‹¤íŒ¨: {len(failed_urls)}ê°œ â†’ Playwrightë¡œ ì¬ì‹œë„")
                other_urls.extend(failed_urls)

        # 3ë‹¨ê³„: ë‚˜ë¨¸ì§€ëŠ” Playwrightë¡œ í¬ë¡¤ë§
        if other_urls:
            print(f"\nğŸ”„ 3ë‹¨ê³„: Swagger/General í¬ë¡¤ë§ (Playwright): {len(other_urls)}ê°œ...")
            pw_results = await self.pw_crawler.crawl_batch(other_urls)
            all_results.extend(pw_results)

            for result in pw_results:
                if result['success']:
                    self.stats['pw_success'] += 1
                else:
                    self.stats['pw_failed'] += 1

            print(f"   âœ… Playwright ì„±ê³µ: {self.stats['pw_success']}ê°œ")
            print(f"   âŒ Playwright ì‹¤íŒ¨: {self.stats['pw_failed']}ê°œ")

        self.stats['total_time'] = time.time() - start_time

        return all_results

    def generate_summary_report(self, results: List[Dict], saved_info: Dict) -> Dict:
        """ìƒì„¸ ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±"""
        # API íƒ€ì…ë³„ ë¶„ë¥˜
        api_types = {}
        for result in results:
            if result.get('success') and result.get('data'):
                api_type = result['data'].get('api_type', 'unknown')
                api_types[api_type] = api_types.get(api_type, 0) + 1
        
        # ë©”ì†Œë“œë³„ ì„±ëŠ¥
        method_performance = {
            'beautifulsoup': {
                'success': self.stats['bs_success'],
                'failed': self.stats.get('bs_failed', 0),
                'success_rate': (
                    f"{(self.stats['bs_success'] / (self.stats['bs_success'] + self.stats.get('bs_failed', 0)) * 100):.1f}%" 
                    if (self.stats['bs_success'] + self.stats.get('bs_failed', 0)) > 0 
                    else '0%'
                )
            },
            'playwright': {
                'success': self.stats['pw_success'],
                'failed': self.stats['pw_failed'],
                'success_rate': (
                    f"{(self.stats['pw_success'] / (self.stats['pw_success'] + self.stats['pw_failed']) * 100):.1f}%"
                    if (self.stats['pw_success'] + self.stats['pw_failed']) > 0
                    else '0%'
                )
            }
        }
        
        summary = {
            'crawling_summary': {
                'total_urls': len(results),
                'total_success': sum(1 for r in results if r.get('success')),
                'total_failed': sum(1 for r in results if not r.get('success')),
                'overall_success_rate': (
                    f"{(sum(1 for r in results if r.get('success')) / len(results) * 100):.1f}%"
                    if results else '0%'
                ),
                'total_time_seconds': round(self.stats['total_time'], 2),
                'avg_time_per_url': round(self.stats['total_time'] / len(results), 2) if results else 0
            },
            'method_performance': method_performance,
            'api_types_found': api_types,
            'save_summary': saved_info,
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'failed_urls': [
                r['url'] for r in results 
                if not r.get('success')
            ],
            'error_details': {
                r['url']: r.get('errors', [])
                for r in results
                if not r.get('success') and r.get('errors')
            }
        }
        
        return summary
    
    async def run(self, urls: List[str], strategy: str = 'optimized'):
        """
        ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜

        Args:
            urls: í¬ë¡¤ë§í•  URL ë¦¬ìŠ¤íŠ¸
            strategy: 'optimized' (LINK ì •ì , ë‚˜ë¨¸ì§€ ë™ì ) or 'fallback' (BS ìš°ì„ ) or 'smart' (íŒ¨í„´ ë¶„ì„)
        """
        print(f"\n{'='*60}")
        print(f"ğŸ¤– í•˜ì´ë¸Œë¦¬ë“œ í¬ë¡¤ëŸ¬ ì‹œì‘")
        print(f"   ì „ëµ: {strategy}")
        print(f"   URL ìˆ˜: {len(urls)}")
        print(f"   ì¶œë ¥ ë””ë ‰í† ë¦¬: {self.output_dir}")
        print(f"   íŒŒì¼ í˜•ì‹: {', '.join(self.formats)}")
        print(f"{'='*60}")

        # í¬ë¡¤ë§ ì‹¤í–‰
        if strategy == 'optimized':
            results = await self.optimized_crawl(urls)
        elif strategy == 'smart':
            results = await self.smart_crawl(urls)
        else:  # fallback
            results = await self.crawl_with_fallback(urls)
        
        # ê²°ê³¼ ì €ì¥
        print("\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘...")
        saved_info = self.save_results(results)
        
        # ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±
        summary = self.generate_summary_report(results, saved_info)
        
        # ìš”ì•½ íŒŒì¼ ì €ì¥
        current_time = datetime.now().strftime('%Y%m%d_%H%M%S')
        summary_file = os.path.join(self.output_dir, f'crawling_summary_{current_time}.json')
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        # ê²°ê³¼ ì¶œë ¥
        self.print_summary(summary)
        
        return results, summary
    
    def print_summary(self, summary: Dict):
        """ìš”ì•½ ì •ë³´ ì¶œë ¥"""
        print(f"\n{'='*60}")
        print("ğŸ“Š í¬ë¡¤ë§ ì™„ë£Œ ìš”ì•½")
        print(f"{'='*60}")
        
        cs = summary['crawling_summary']
        print(f"\nğŸ“ˆ ì „ì²´ í†µê³„:")
        print(f"   - ì´ URL: {cs['total_urls']}ê°œ")
        print(f"   - ì„±ê³µ: {cs['total_success']}ê°œ")
        print(f"   - ì‹¤íŒ¨: {cs['total_failed']}ê°œ")
        print(f"   - ì„±ê³µë¥ : {cs['overall_success_rate']}")
        print(f"   - ì†Œìš” ì‹œê°„: {cs['total_time_seconds']}ì´ˆ")
        print(f"   - í‰ê·  ì²˜ë¦¬ ì‹œê°„: {cs['avg_time_per_url']}ì´ˆ/URL")
        
        print(f"\nğŸ”§ ë©”ì†Œë“œë³„ ì„±ëŠ¥:")
        mp = summary['method_performance']
        print(f"   BeautifulSoup:")
        print(f"      - ì„±ê³µ: {mp['beautifulsoup']['success']}ê°œ")
        print(f"      - ì„±ê³µë¥ : {mp['beautifulsoup']['success_rate']}")
        print(f"   Playwright:")
        print(f"      - ì„±ê³µ: {mp['playwright']['success']}ê°œ")
        print(f"      - ì‹¤íŒ¨: {mp['playwright']['failed']}ê°œ")
        print(f"      - ì„±ê³µë¥ : {mp['playwright']['success_rate']}")
        
        if summary['api_types_found']:
            print(f"\nğŸ“¦ API íƒ€ì…ë³„ ë¶„í¬:")
            for api_type, count in summary['api_types_found'].items():
                print(f"   - {api_type}: {count}ê°œ")
        
        print(f"\nğŸ’¾ ì €ì¥ ê²°ê³¼:")
        ss = summary['save_summary']
        print(f"   - ì €ì¥ ì„±ê³µ: {ss['total_saved']}ê°œ")
        print(f"   - ì €ì¥ ì‹¤íŒ¨: {ss['failed_saves']}ê°œ")
        print(f"   - ìƒì„± íŒŒì¼: {len(ss['saved_files'])}ê°œ")
        
        if summary['failed_urls']:
            print(f"\nâš ï¸ ì‹¤íŒ¨ URL: {len(summary['failed_urls'])}ê°œ")
            if len(summary['failed_urls']) <= 5:
                for url in summary['failed_urls']:
                    print(f"   - {url}")
            else:
                for url in summary['failed_urls'][:5]:
                    print(f"   - {url}")
                print(f"   ... ì™¸ {len(summary['failed_urls'])-5}ê°œ")
        
        print(f"\nâœ… ìš”ì•½ íŒŒì¼ ì €ì¥: {self.output_dir}/crawling_summary.json")
        print(f"{'='*60}\n")


def generate_urls_from_numbers(numbers: List[int]) -> List[str]:
    """ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸ë¡œ URL ìƒì„±"""
    base_url = "https://www.data.go.kr/data/{}/openapi.do"
    return [base_url.format(num) for num in numbers]

def generate_urls(start_num: int, end_num: int) -> List[str]:
    """ë²ˆí˜¸ ë²”ìœ„ë¡œ URL ìƒì„±"""
    base_url = "https://www.data.go.kr/data/{}/openapi.do"
    return [base_url.format(num) for num in range(start_num, end_num + 1)]

def check_metadata_and_get_valid_numbers(start_num: int, end_num: int) -> List[int]:
    """ë©”íƒ€ë°ì´í„° ìŠ¤ìº”ìœ¼ë¡œ ìœ íš¨ ë²ˆí˜¸ í™•ì¸"""
    print(f"\nğŸ” ë©”íƒ€ë°ì´í„° ìŠ¤ìº” ì‹œì‘: {start_num} ~ {end_num}")
    scanner = OpenAPIMetadataScanner(
        start_num=start_num, 
        end_num=end_num, 
        max_workers=150
    )
    results = scanner.scan_range()
    scanner.save_results()
    scanner.print_summary()
    
    valid_numbers = results['data_numbers']
    print(f"\nâœ… ë©”íƒ€ë°ì´í„° ìŠ¤ìº” ì™„ë£Œ! ìœ íš¨ ë²ˆí˜¸: {len(valid_numbers)}ê°œ")
    return valid_numbers

async def main():
    parser = argparse.ArgumentParser(
        description='í•˜ì´ë¸Œë¦¬ë“œ API í¬ë¡¤ëŸ¬ (BeautifulSoup + Playwright)',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì œ:
  # ê¸°ë³¸ ì‚¬ìš© (optimized ì „ëµ - LINK ì •ì , ë‚˜ë¨¸ì§€ ë™ì )
  python main_openapi.py -s 1000 -e 1100

  # Fallback ì „ëµ (ëª¨ë“  URLì„ BS ìš°ì„  ì‹œë„)
  python main_openapi.py -s 1000 -e 1100 --strategy fallback

  # Smart ì „ëµ (URL íŒ¨í„´ ë¶„ì„)
  python main_openapi.py -s 1000 -e 1100 --strategy smart

  # ë©”íƒ€ë°ì´í„° ìŠ¤ìº” ê±´ë„ˆë›°ê¸°
  python main_openapi.py -s 1000 -e 1100 --skip-metadata

  # íŠ¹ì • í˜•ì‹ë§Œ ì €ì¥
  python main_openapi.py -s 1000 -e 1100 --formats json xml
        """
    )

    parser.add_argument('-s', '--start', type=int, required=True,
                       help='ì‹œì‘ ë¬¸ì„œ ë²ˆí˜¸')
    parser.add_argument('-e', '--end', type=int, required=True,
                       help='ë ë¬¸ì„œ ë²ˆí˜¸')
    parser.add_argument('-o', '--output-dir',
                       default='./data',
                       help='ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: ./data)')
    parser.add_argument('--formats', nargs='+',
                       default=['json', 'xml', 'csv'],
                       choices=['json', 'xml', 'csv'],
                       help='ì €ì¥í•  íŒŒì¼ í˜•ì‹ (ê¸°ë³¸ê°’: ëª¨ë“  í˜•ì‹)')
    parser.add_argument('-w', '--workers', type=int, default=30,
                       help='ë™ì‹œ ì‘ì—…ì ìˆ˜ (ê¸°ë³¸ê°’: 30)')
    parser.add_argument('--skip-metadata', action='store_true',
                       help='ë©”íƒ€ë°ì´í„° ìŠ¤ìº” ê±´ë„ˆë›°ê¸°')
    parser.add_argument('--strategy', choices=['optimized', 'fallback', 'smart'],
                       default='optimized',
                       help='í¬ë¡¤ë§ ì „ëµ (optimized: LINKì •ì /ë‚˜ë¨¸ì§€ë™ì , fallback: BSìš°ì„ , smart: íŒ¨í„´ë¶„ì„)')
    
    args = parser.parse_args()
    
    # ìœ íš¨ì„± ê²€ì‚¬
    if args.start > args.end:
        print("âŒ ì˜¤ë¥˜: ì‹œì‘ ë²ˆí˜¸ê°€ ë ë²ˆí˜¸ë³´ë‹¤ í´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    if args.workers < 5 or args.workers > 40:
        print(f"âš ï¸ ê²½ê³ : ì‘ì—…ì ìˆ˜ë¥¼ 5-40 ì‚¬ì´ë¡œ ì¡°ì •í•©ë‹ˆë‹¤. (ì…ë ¥ê°’: {args.workers})")
        args.workers = max(5, min(40, args.workers))
    
    # URL ìƒì„±
    if args.skip_metadata:
        print("âš ï¸ ë©”íƒ€ë°ì´í„° ìŠ¤ìº”ì„ ê±´ë„ˆë›°ê³  ëª¨ë“  ë²ˆí˜¸ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤.")
        urls = generate_urls(args.start, args.end)
    else:
        valid_numbers = check_metadata_and_get_valid_numbers(args.start, args.end)
        if not valid_numbers:
            print("âŒ ìœ íš¨í•œ ë²ˆí˜¸ê°€ ì—†ìŠµë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")
            return
        urls = generate_urls_from_numbers(valid_numbers)
    
    # í¬ë¡¤ëŸ¬ ì‹¤í–‰
    crawler = HybridCrawler(
        output_dir=args.output_dir,
        formats=args.formats,
        max_workers=args.workers
    )
    
    await crawler.run(urls, strategy=args.strategy)

if __name__ == '__main__':
    asyncio.run(main())