import asyncio
import argparse
import os
from datetime import datetime
from tqdm import tqdm
from parser import DataExporter
from metadata_openapi import OpenAPIMetadataScanner
import json
from playwright.async_api import async_playwright
import time
import re

# ë¬¸ìì—´ ì •ì œ í•¨ìˆ˜: ëª¨ë“  ì´ìŠ¤ì¼€ì´í”„ ë¬¸ì ë° HTML íƒœê·¸ ì™„ì „ ì œê±°
def clean_text(text):
    if not isinstance(text, str):
        return text
    # \n, \r, \t ë“± ëª¨ë“  ì´ìŠ¤ì¼€ì´í”„ ë¬¸ì â†’ ê³µë°±
    text = re.sub(r'[\n\r\t]+', ' ', text)
    # HTML íƒœê·¸ ì œê±°
    text = re.sub(r'<[^>]+>', '', text)
    # ì—°ì†ëœ ê³µë°± â†’ í•œ ì¹¸
    text = re.sub(r' +', ' ', text)
    text = text.strip()
    return text

# dict/list ì „ì²´ì— clean_text ì¬ê·€ ì ìš© í•¨ìˆ˜

def clean_all_text(obj):
    if isinstance(obj, dict):
        return {k: clean_all_text(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [clean_all_text(v) for v in obj]
    elif isinstance(obj, str):
        return clean_text(obj)
    else:
        return obj

# Playwright ê¸°ë°˜ í…Œì´ë¸”/Swagger/ì¼ë°˜ API ì •ë³´ ì¶”ì¶œ í•¨ìˆ˜
async def extract_api_info(page, url):
    result = {
        'success': False,
        'data': None,
        'saved_files': [],
        'errors': [],
        'api_id': None,
        'url': url
    }
    try:
        await page.goto(url, timeout=15000)
        await page.wait_for_selector('body', timeout=7000)
        # --- í…Œì´ë¸” ì •ë³´ ì¶”ì¶œ (parser.pyì˜ extract_table_infoë¥¼ playwrightë¡œ ë³€í™˜) ---
        table_info = {}
        tables = await page.query_selector_all('table.dataset-table')
        for table in tables:
            rows = await table.query_selector_all('tr')
            for row in rows:
                try:
                    th = await row.query_selector('th')
                    td = await row.query_selector('td')
                    if th and td:
                        key = clean_text(await th.inner_text())
                        value = clean_text(await td.inner_text())
                        if 'ì „í™”ë²ˆí˜¸' in key:
                            tel_div = await td.query_selector('#telNoDiv')
                            if tel_div:
                                value = clean_text(await tel_div.inner_text())
                        if not value:
                            link = await td.query_selector('a')
                            if link:
                                value = clean_text(await link.inner_text())
                        if key and value:
                            table_info[key] = value
                except:
                    continue
        # API ID ì¶”ì¶œ
        m = re.search(r'/data/(\d+)/openapi', url)
        api_id = m.group(1) if m else f"api_{url.replace('https://', '').replace('/', '_')}"
        result['api_id'] = api_id
        # API ìœ í˜• í™•ì¸
        api_type_field = table_info.get('API ìœ í˜•', '').upper()
        # LINK íƒ€ì… ì²˜ë¦¬
        if 'LINK' in api_type_field:
            result['data'] = {
                'api_id': api_id,
                'crawled_url': url,
                'crawled_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'info': table_info,
                'api_type': 'link',
                'skip_reason': 'LINK íƒ€ì… APIëŠ” í…Œì´ë¸” ì •ë³´ë§Œ ìˆ˜ì§‘'
            }
            result['success'] = True
            return result
        # Swagger JSON ì¶”ì¶œ (JS ë³€ìˆ˜/ìŠ¤í¬ë¦½íŠ¸ ë“±)
        swagger_json = None
        # 1. window ì „ì—­ swaggerJson ë³€ìˆ˜ ì‹œë„
        swagger_json = await page.evaluate('''() => {
            try {
                if (typeof swaggerJson !== 'undefined' && swaggerJson !== null) {
                    if (typeof swaggerJson === 'string') {
                        if (swaggerJson.trim() === '') return null;
                        return JSON.parse(swaggerJson);
                    } else if (typeof swaggerJson === 'object') {
                        return swaggerJson;
                    }
                }
                return null;
            } catch (e) { return null; }
        }''')
        # 2. script íƒœê·¸ì—ì„œ ì§ì ‘ íŒŒì‹± (ì…€ë ˆë‹ˆì›€ ë°©ì‹)
        if not (swagger_json and isinstance(swagger_json, dict) and swagger_json):
            scripts = await page.query_selector_all('script')
            import re as _re, json as _json
            for script in scripts:
                try:
                    content = await script.inner_text()
                    # ê°ì²´ íŒ¨í„´
                    match = _re.search(r'var\s+swaggerJson\s*=\s*(\{.*?\})\s*;', content, _re.DOTALL)
                    if match:
                        try:
                            swagger_json = _json.loads(match.group(1))
                            break
                        except Exception:
                            continue
                    # ë¬¸ìì—´ íŒ¨í„´ (ë°±í‹±)
                    match2 = _re.search(r'var\s+swaggerJson\s*=\s*`(\{.*?\})`', content, _re.DOTALL)
                    if match2:
                        try:
                            swagger_json = _json.loads(match2.group(1))
                            break
                        except Exception:
                            continue
                    # ë¬¸ìì—´ íŒ¨í„´ (ë”°ì˜´í‘œ)
                    match3 = _re.search(r'var\s+swaggerJson\s*=\s*"(\{.*?\})"', content, _re.DOTALL)
                    if match3:
                        try:
                            swagger_json = _json.loads(match3.group(1))
                            break
                        except Exception:
                            continue
                except Exception:
                    continue
        if swagger_json and isinstance(swagger_json, dict) and swagger_json:
            # Swagger API
            from parser import NaraParser
            parser = NaraParser(None)  # driver ì—†ì´ ì‚¬ìš©
            api_info = parser.extract_api_info(swagger_json)
            base_url = parser.extract_base_url(swagger_json)
            api_info['base_url'] = base_url
            api_info['schemes'] = swagger_json.get('schemes', ['https'])
            endpoints = parser.extract_endpoints(swagger_json)
            result['data'] = {
                'api_id': api_id,
                'crawled_url': url,
                'crawled_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'info': table_info,
                'api_info': api_info,
                'endpoints': endpoints,
                'swagger_json': swagger_json,
                'api_type': 'swagger'
            }
            result['success'] = True
            return result
        # ì¼ë°˜ API ì •ë³´ ì¶”ì¶œ (ìƒì„¸ê¸°ëŠ¥/ìš”ì²­ë³€ìˆ˜/ì¶œë ¥ê²°ê³¼)
        # (ê°„ë‹¨í™”: ìƒì„¸ê¸°ëŠ¥ div, ìš”ì²­ë³€ìˆ˜/ì¶œë ¥ê²°ê³¼ í…Œì´ë¸” ë“±)
        general_api_info = {}
        # ìƒì„¸ê¸°ëŠ¥
        try:
            detail_div = await page.query_selector('#open-api-detail-result')
            if detail_div:
                desc_elem = await detail_div.query_selector('h4.tit')
                description = (await desc_elem.inner_text()).strip() if desc_elem else ''
                general_api_info['detail_info'] = {'description': description}
        except:
            pass
        # ìš”ì²­ë³€ìˆ˜/ì¶œë ¥ê²°ê³¼ëŠ” ìƒëµ(í•„ìš”ì‹œ parser.py ì°¸ê³ í•´ ì¶”ê°€)
        if general_api_info:
            result['data'] = {
                'api_id': api_id,
                'crawled_url': url,
                'crawled_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'info': table_info,
                'general_api_info': general_api_info,
                'api_type': 'general'
            }
            result['success'] = True
            return result
        # ì •ë³´ ë¶€ì¡±
        result['errors'].append('API ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ')
        return result
    except Exception as e:
        result['errors'].append(f'í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}')
        return result

# ë‹¨ì¼ URL í¬ë¡¤ë§ (ì»¨í…ìŠ¤íŠ¸ í’€ êµ¬ì¡°)
async def crawl_url_playwright(browser, url, output_dir, formats, semaphore, timing_results):
    async with semaphore:
        start_time = time.time()
        context = await browser.new_context()
        page = await context.new_page()
        result = await extract_api_info(page, url)
        await context.close()
        elapsed = time.time() - start_time
        timing_results[url] = elapsed
        # ì €ì¥
        if result['success']:
            # clean_all_text ì ìš©
            result['data'] = clean_all_text(result['data'])
            saved_files, save_errors = DataExporter.save_crawling_result(result['data'], output_dir, result['api_id'], formats)
            result['saved_files'] = saved_files
            result['errors'].extend(save_errors)
        return result

# ë³‘ë ¬ ë°°ì¹˜ í¬ë¡¤ë§ (ë¸Œë¼ìš°ì € 1ê°œë§Œ ìƒì„±)
async def batch_crawl_playwright(urls, output_dir, formats, max_workers):
    semaphore = asyncio.Semaphore(max_workers)
    timing_results = {}
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        tasks = [crawl_url_playwright(browser, url, output_dir, formats, semaphore, timing_results) for url in urls]
        results = []
        with tqdm(asyncio.as_completed(tasks), total=len(tasks), desc="í¬ë¡¤ë§ ì§„í–‰") as pbar:
            for f in pbar:
                res = await f
                results.append(res)
                # í‰ê·  ì†Œìš” ì‹œê°„ í‘œì‹œ
                if timing_results:
                    avg_time = sum(timing_results.values()) / len(timing_results)
                    pbar.set_postfix({'í‰ê· ì†Œìš”(s)': f'{avg_time:.1f}'})
        await browser.close()
    # ìš”ì•½ ì €ì¥
    summary = {
        'total': len(urls),
        'success': sum(1 for r in results if r['success']),
        'failed': sum(1 for r in results if not r['success']),
        'start_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'end_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'success_rate': f"{(sum(1 for r in results if r['success']) / len(urls) * 100):.1f}%" if urls else '0%',
        'timing_per_url': timing_results
    }
    os.makedirs(output_dir, exist_ok=True)
    # clean_all_text ì ìš©
    summary_cleaned = clean_all_text(summary)
    with open(os.path.join(output_dir, 'crawling_summary.json'), 'w', encoding='utf-8') as f:
        json.dump(summary_cleaned, f, ensure_ascii=False, indent=2)
    return results

def generate_urls_from_numbers(numbers):
    base_url = "https://www.data.go.kr/data/{}/openapi.do"
    return [base_url.format(num) for num in numbers]

def generate_urls(start_num, end_num):
    base_url = "https://www.data.go.kr/data/{}/openapi.do"
    return [base_url.format(num) for num in range(start_num, end_num + 1)]

def check_metadata_and_get_valid_numbers(start_num, end_num):
    print(f"\nğŸ” ë©”íƒ€ë°ì´í„° ìŠ¤ìº” ì‹œì‘: {start_num} ~ {end_num}")
    scanner = OpenAPIMetadataScanner(start_num=start_num, end_num=end_num, max_workers=50)
    results = scanner.scan_range()
    scanner.save_results()
    scanner.print_summary()
    valid_numbers = results['file_numbers']
    print(f"\nâœ… ë©”íƒ€ë°ì´í„° ìŠ¤ìº” ì™„ë£Œ! ìœ íš¨ ë²ˆí˜¸: {len(valid_numbers)}ê°œ")
    return valid_numbers

def main():
    parser = argparse.ArgumentParser(description='ë‚˜ë¼ì¥í„° API í¬ë¡¤ëŸ¬ (Playwright ê¸°ë°˜)')
    parser.add_argument('-s', '--start', type=int, required=True, help='ì‹œì‘ ë¬¸ì„œ ë²ˆí˜¸')
    parser.add_argument('-e', '--end', type=int, required=True, help='ë ë¬¸ì„œ ë²ˆí˜¸')
    parser.add_argument('-o', '--output-dir', default='/data/download_openapi', help='ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: /data/download_openapi)')
    parser.add_argument('--formats', nargs='+', default=['json', 'xml', 'md', 'csv'],
                      choices=['json', 'xml', 'md', 'csv'], help='ì €ì¥í•  íŒŒì¼ í˜•ì‹')
    parser.add_argument('-w', '--workers', type=int, default=15, help='ë™ì‹œ ì‘ì—…ì ìˆ˜ (ê¸°ë³¸ê°’: 15, í—ˆìš©ë²”ìœ„: 10~20)')
    parser.add_argument('--skip-metadata', action='store_true', help='ë©”íƒ€ë°ì´í„° ìŠ¤ìº” ê±´ë„ˆë›°ê¸° (ëª¨ë“  ë²ˆí˜¸ í¬ë¡¤ë§)')
    args = parser.parse_args()
    if args.start > args.end:
        print("âŒ ì‹œì‘ ë²ˆí˜¸ê°€ ë ë²ˆí˜¸ë³´ë‹¤ í´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    if args.workers < 10 or args.workers > 20:
        print("âš ï¸ ë™ì‹œ ì‘ì—…ì ìˆ˜ëŠ” 10~20 ì‚¬ì´ë¡œ ì„¤ì •í•´ì£¼ì„¸ìš”. ê¸°ë³¸ê°’(15)ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
        args.workers = 15
    if args.skip_metadata:
        print("âš ï¸ ë©”íƒ€ë°ì´í„° ìŠ¤ìº”ì„ ê±´ë„ˆë›°ê³  ëª¨ë“  ë²ˆí˜¸ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤.")
        urls = generate_urls(args.start, args.end)
    else:
        valid_numbers = check_metadata_and_get_valid_numbers(args.start, args.end)
        if not valid_numbers:
            print("âŒ ìœ íš¨í•œ ë²ˆí˜¸ê°€ ì—†ìŠµë‹ˆë‹¤. í¬ë¡¤ë§ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            return
        urls = generate_urls_from_numbers(valid_numbers)
    asyncio.run(batch_crawl_playwright(urls, args.output_dir, args.formats, args.workers))

if __name__ == '__main__':
    main() 