# -*- coding: utf-8 -*-
import requests
import json
import os
import argparse
from datetime import datetime
import sys
from tqdm import tqdm
import concurrent.futures
import time
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException


class OdcloudCrawler:
    """infuser.odcloud.kr API crawler with table info extraction capability"""
    
    def __init__(self, timeout=30):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        self.timeout = timeout
        self.driver = None
        # Ìï≠ÏÉÅ driverÎ•º ÏÑ§Ï†ïÌïòÏó¨ ÌÖåÏù¥Î∏î Ï†ïÎ≥¥ Ï∂îÏ∂ú Í∞ÄÎä•ÌïòÎèÑÎ°ù Ìï®
        self._setup_driver()
    
    def _setup_driver(self):
        """Setup Chrome driver for table info extraction"""
        try:
            chrome_options = Options()
            chrome_options.add_argument("--headless")
            chrome_options.add_argument("--no-sandbox")
            chrome_options.add_argument("--disable-dev-shm-usage")
            chrome_options.add_argument("--disable-gpu")
            
            self.driver = webdriver.Chrome(options=chrome_options)
            
        except Exception as e:
            print(f"Failed to setup Chrome driver: {e}")
            self.driver = None
    

    

    
    def extract_table_info(self, namespace_id):
        """ÌÖåÏù¥Î∏î Ï†ïÎ≥¥ Ï∂îÏ∂ú - dataset-tableÍ≥º fileDataDetail Ìëú ÏöîÏÜåÏóêÏÑú ÌÇ§:Í∞í ÌòïÌÉúÎ°ú Ï∂îÏ∂ú"""
        try:
            if not self.driver:
                return {}
            
            table_info = {}
            
            # data.go.kr ÏÉÅÏÑ∏ ÌéòÏù¥ÏßÄ URL (openapi ÌÉ≠ Ìè¨Ìï®)
            detail_url = f"https://www.data.go.kr/data/{namespace_id}/fileData.do#tab-layer-openapi"
            
            try:
                # ÏÉÅÏÑ∏ ÌéòÏù¥ÏßÄ Î∞©Î¨∏
                self.driver.get(detail_url)
                
                # ÌéòÏù¥ÏßÄ Î°úÎî© ÎåÄÍ∏∞
                WebDriverWait(self.driver, 10).until(
                    EC.presence_of_element_located((By.TAG_NAME, "body"))
                )
                
                # ÎèôÏ†Å ÏΩòÌÖêÏ∏† Î°úÎî© ÎåÄÍ∏∞
                time.sleep(2)
                
                # Ï†ïÌôïÌïú ÌÖåÏù¥Î∏î Ï∞æÍ∏∞ (ÌÖåÏä§Ìä∏ Í≤∞Í≥º Í∏∞Î∞ò)
                table_selectors = [
                    "table.dataset-table.fileDataDetail#apiDetailTableArea-pc",  # Ï†ïÌôïÌïú ÏÑ†ÌÉùÏûê
                    "table#apiDetailTableArea-pc",  # IDÎßåÏúºÎ°ú
                    "table.dataset-table.fileDataDetail",  # ÌÅ¥ÎûòÏä§ÎßåÏúºÎ°ú
                    "table.dataset-table",  # parser.py Î∞©Ïãù
                    "table"  # Î™®Îì† ÌÖåÏù¥Î∏î (ÎßàÏßÄÎßâ ÏàòÎã®)
                ]
                
                tables = []
                used_selector = ""
                
                for selector in table_selectors:
                    try:
                        tables = self.driver.find_elements(By.CSS_SELECTOR, selector)
                        if tables:
                            used_selector = selector
                            print(f"ÌÖåÏù¥Î∏î Ï∞æÏùå ({namespace_id}): {selector} ({len(tables)}Í∞ú)")
                            break
                    except Exception as e:
                        continue
                
                if tables:
                    print(f"ÌÖåÏù¥Î∏î Ï∞æÏùå ({namespace_id}): table.dataset-table ({len(tables)}Í∞ú)")
                    # Î™®Îì† ÌÖåÏù¥Î∏îÏóêÏÑú Ï†ïÎ≥¥ Ï∂îÏ∂ú
                    for table in tables:
                        # ÌÖåÏù¥Î∏î ÎÇ¥Ïö© Ï∂îÏ∂ú
                        rows = table.find_elements(By.TAG_NAME, "tr")
                        
                        for row in rows:
                            try:
                                # thÏôÄ td ÏÖÄÏùÑ Î∂ÑÎ¶¨Ìï¥ÏÑú Ï∞æÍ∏∞
                                th_cells = row.find_elements(By.TAG_NAME, "th")
                                td_cells = row.find_elements(By.TAG_NAME, "td")
                                
                                # th-td ÏåçÏúºÎ°ú Ï≤òÎ¶¨
                                for i in range(min(len(th_cells), len(td_cells))):
                                    key_cell = th_cells[i]
                                    value_cell = td_cells[i]
                                    
                                    # ÌÇ§ Ï∂îÏ∂ú
                                    key = key_cell.get_attribute("textContent").strip()
                                    
                                    # Í∞í Ï∂îÏ∂ú
                                    value = value_cell.get_attribute("textContent").strip()
                                    
                                    # Ï†ÑÌôîÎ≤àÌò∏Ïùò Í≤ΩÏö∞ span ÌÉúÍ∑∏ÏóêÏÑú Ï∂îÏ∂ú
                                    if "Ï†ÑÌôîÎ≤àÌò∏" in key:
                                        try:
                                            span = value_cell.find_element(By.TAG_NAME, "span")
                                            value = span.get_attribute("textContent").strip()
                                        except:
                                            pass
                                    
                                    # ÎßÅÌÅ¨Í∞Ä ÏûàÎäî Í≤ΩÏö∞ ÎßÅÌÅ¨ ÌÖçÏä§Ìä∏Îßå Ï∂îÏ∂ú
                                    if not value:
                                        try:
                                            link = value_cell.find_element(By.TAG_NAME, "a")
                                            value = link.get_attribute("textContent").strip()
                                        except:
                                            pass
                                    
                                    if key and value:
                                        table_info[key] = value
                                            
                            except Exception as e:
                                continue
                else:
                    print(f"ÌÖåÏù¥Î∏îÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏùå ({namespace_id})")
                    return {}
                
                return table_info
                
            except Exception as e:
                print(f"ÌÖåÏù¥Î∏î Ï†ïÎ≥¥ Ï∂îÏ∂ú Ï§ë Ïò§Î•ò Î∞úÏÉù ({namespace_id}): {str(e)}")
                return {}
                
        except Exception as e:
            print(f"ÌÖåÏù¥Î∏î Ï†ïÎ≥¥ Ï∂îÏ∂ú Ïã§Ìå® ({namespace_id}): {str(e)}")
            return {}
    

    
    def crawl_namespace(self, namespace_id):
        """Crawl OAS document for specific namespace"""
        url = f"https://infuser.odcloud.kr/oas/docs?namespace={namespace_id}/v1"
        
        try:
            response = self.session.get(url, timeout=self.timeout)
            response.raise_for_status()
            
            # Check Content-Type
            content_type = response.headers.get('content-type', '')
            
            # ÌÖåÏù¥Î∏î Ï†ïÎ≥¥ Ï∂îÏ∂ú (Ìï≠ÏÉÅ ÏàòÌñâ)
            table_info = {}
            if self.driver:
                table_info = self.extract_table_info(namespace_id)
            
            if 'application/json' in content_type:
                # JSON response
                result = {
                    'success': True,
                    'data': response.json(),
                    'namespace_id': namespace_id,
                    'url': url,
                    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    'status_code': response.status_code
                }
                
                # ÌÖåÏù¥Î∏î Ï†ïÎ≥¥Í∞Ä ÏûàÏúºÎ©¥ Ï∂îÍ∞Ä
                if table_info and len(table_info) > 0:
                    result['table_info'] = table_info
                
                return result
            else:
                # Non-JSON response - save as text
                result = {
                    'success': True,
                    'data': response.text,
                    'namespace_id': namespace_id,
                    'url': url,
                    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    'status_code': response.status_code,
                    'content_type': content_type
                }
                
                # ÌÖåÏù¥Î∏î Ï†ïÎ≥¥Í∞Ä ÏûàÏúºÎ©¥ Ï∂îÍ∞Ä
                if table_info and len(table_info) > 0:
                    result['table_info'] = table_info
                
                return result
                
        except requests.exceptions.RequestException as e:
            return {
                'success': False,
                'error': str(e),
                'namespace_id': namespace_id,
                'url': url,
                'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
    
    def close(self):
        """Close session and driver"""
        self.session.close()
        if self.driver:
            try:
                self.driver.quit()
            except:
                pass

def save_result(result, output_dir):
    """Save result to JSON file"""
    os.makedirs(output_dir, exist_ok=True)
    
    namespace_id = result['namespace_id']
    filename = f"{namespace_id}.json"
    filepath = os.path.join(output_dir, filename)
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    return filepath

def generate_namespace_range(start_num, end_num):
    """Generate namespace IDs between start and end numbers"""
    return [str(num) for num in range(start_num, end_num + 1)]

def batch_crawl(namespace_ids, output_dir="download_fileData_api", max_workers=10):
    """Batch crawling with optional table info extraction"""
    total_count = len(namespace_ids)
    
    print(f"\nOdcloud OAS Document Crawling Started")
    print(f"   Total {total_count} namespaces")
    print(f"   Workers: {max_workers}")
    print(f"   Output directory: {output_dir}")
    
    # Result storage
    results = {
        'total': total_count,
        'success': 0,
        'failed': 0,
        'table_extracted': 0,
        'table_failed': 0,
        'failed_namespaces': [],
        'start_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    }
    
    crawler = OdcloudCrawler()
    
    def crawl_single_namespace(namespace_id):
        """Crawl single namespace with optional table info extraction"""
        try:
            result = crawler.crawl_namespace(namespace_id)
            
            if result['success']:
                saved_file = save_result(result, output_dir)
                print(f"SUCCESS {namespace_id} - Saved JSON: {saved_file}")
                
                # ÌÖåÏù¥Î∏î Ï†ïÎ≥¥ Ï∂îÏ∂ú Í≤∞Í≥º Î°úÍπÖ
                if 'table_info' in result and result['table_info'] and len(result['table_info']) > 0:
                    table_count = len(result['table_info'])
                    print(f"TABLE INFO {namespace_id} - Extracted {table_count} items")
                else:
                    print(f"TABLE INFO {namespace_id} - No table info available (table not found or empty)")
                
                has_table_info = 'table_info' in result and len(result.get('table_info', {})) > 0
                return {'json': True, 'table': has_table_info}
            else:
                print(f"FAILED {namespace_id} - Error: {result.get('error', 'Unknown error')}")
                return {'json': False, 'table': False}
        except Exception as e:
            print(f"ERROR {namespace_id} - Exception: {str(e)}")
            return {'json': False, 'table': False}
    
    try:
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_namespace = {
                executor.submit(crawl_single_namespace, namespace_id): namespace_id 
                for namespace_id in namespace_ids
            }
            
            with tqdm(total=total_count, desc="Crawling Progress") as pbar:
                for future in concurrent.futures.as_completed(future_to_namespace):
                    namespace_id = future_to_namespace[future]
                    
                    try:
                        result = future.result()
                        if isinstance(result, dict):
                            if result.get('json'):
                                results['success'] += 1
                            else:
                                results['failed'] += 1
                                results['failed_namespaces'].append(namespace_id)
                            
                            # Count table extraction results
                            if result.get('table'):
                                results['table_extracted'] += 1
                            elif result.get('table') is False:
                                results['table_failed'] += 1
                        else:
                            # Legacy boolean result
                            if result:
                                results['success'] += 1
                            else:
                                results['failed'] += 1
                                results['failed_namespaces'].append(namespace_id)
                    except Exception as e:
                        results['failed'] += 1
                        results['failed_namespaces'].append(namespace_id)
                        print(f"\nException occurred: {namespace_id} - {str(e)}")
                    
                    pbar.update(1)
                    time.sleep(0.1)  # Prevent server overload
    
    finally:
        crawler.close()
    
    # Results summary
    results['end_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    results['success_rate'] = f"{(results['success'] / total_count * 100):.1f}%" if total_count > 0 else "0%"
    results['table_success_rate'] = f"{(results['table_extracted'] / total_count * 100):.1f}%" if total_count > 0 else "0%"
    
    # Save results
    os.makedirs(output_dir, exist_ok=True)
    summary_file = os.path.join(output_dir, "crawling_summary.json")
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    # Save failed namespaces list
    if results['failed_namespaces']:
        failed_file = os.path.join(output_dir, "failed_namespaces.txt")
        with open(failed_file, 'w', encoding='utf-8') as f:
            for namespace_id in results['failed_namespaces']:
                f.write(f"{namespace_id}\n")
    
    # Final results output
    print("\n" + "=" * 50)
    print("Odcloud OAS Document Crawling Completed!")
    print("=" * 50)
    print(f"Overall Results:")
    print(f"   Total processed: {results['total']} namespaces")
    print(f"   JSON Success: {results['success']} ({results['success_rate']})")
    print(f"   JSON Failed: {results['failed']}")
    print(f"   Table Info Extracted: {results['table_extracted']} ({results.get('table_success_rate', '0%')})")
    print(f"   Table Info Failed: {results['table_failed']}")
    print(f"   Results location: {output_dir}")
    print(f"   Summary file: crawling_summary.json")
    if results['failed']:
        print(f"   Failed list: failed_namespaces.txt")

def check_metadata_and_get_valid_numbers(start_num, end_num, scan_type='fileData'):
    """Î©îÌÉÄÎç∞Ïù¥ÌÑ∞Î•º Ï≤¥ÌÅ¨ÌïòÏó¨ Ïú†Ìö®Ìïú Î≤àÌò∏Îì§Îßå Î∞òÌôò"""
    print(f"\nüîç Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Ïä§Ï∫î ÏãúÏûë: {start_num} ~ {end_num}")
    
    # Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Ïä§Ï∫êÎÑà ÏÉùÏÑ±
    from metadata_fileData import FileDataMetadataScanner
    scanner = FileDataMetadataScanner(
        start_num=start_num,
        end_num=end_num,
        max_workers=50
    )
    
    # Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Ïä§Ï∫î Ïã§Ìñâ
    results = scanner.scan_range()
    
    # Í≤∞Í≥º Ï†ÄÏû•
    scanner.save_results()
    
    # ÏöîÏïΩ Ï∂úÎ†•
    scanner.print_summary()
    
    # ÌååÏùº Îç∞Ïù¥ÌÑ∞Í∞Ä ÏûàÎäî Î≤àÌò∏Îì§ Î∞òÌôò
    valid_numbers = results['file_numbers']
    
    print(f"\n‚úÖ Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Ïä§Ï∫î ÏôÑÎ£å!")
    print(f"   üìã Ï†ÑÏ≤¥ Ïä§Ï∫î: {results['total']}Í∞ú")
    print(f"   ‚úÖ Ïú†Ìö®Ìïú Î≤àÌò∏: {len(valid_numbers)}Í∞ú")
    print(f"   üìä ÌïÑÌÑ∞ÎßÅ ÎπÑÏú®: {(len(valid_numbers) / results['total'] * 100):.1f}%")
    
    return valid_numbers

def main():
    """Main function"""
    parser = argparse.ArgumentParser(description='Odcloud OAS Document Crawler with Metadata Check')
    parser.add_argument('-s', '--start', type=int, required=True, help='Start document number')
    parser.add_argument('-e', '--end', type=int, required=True, help='End document number')
    parser.add_argument('-o', '--output-dir', default='download_fileData_api', help='Output directory (default: download_fileData_api)')
    parser.add_argument('-w', '--workers', type=int, default=10, help='Number of workers (default: 10)')
    parser.add_argument('--timeout', type=int, default=30, help='Request timeout in seconds (default: 30)')
    parser.add_argument('--skip-metadata', action='store_true', help='Skip metadata check and crawl all numbers')
    parser.add_argument('--scan-type', choices=['openapi', 'fileData', 'standard'], default='fileData',
                      help='Metadata scan type (default: fileData)')

    
    args = parser.parse_args()
    
    # Input validation
    if args.start > args.end:
        print("ERROR: Start number cannot be greater than end number.")
        sys.exit(1)
    
    if args.workers < 1 or args.workers > 50:
        print("WARNING: Number of workers should be between 1-50. Setting to 10.")
        args.workers = 10
    

    
    # Metadata check and filtering
    if args.skip_metadata:
        print("‚ö†Ô∏è Skipping metadata check and crawling all numbers.")
        namespace_ids = generate_namespace_range(args.start, args.end)
    else:
        # Check metadata first to get valid numbers
        valid_numbers = check_metadata_and_get_valid_numbers(
            args.start, args.end, args.scan_type
        )
        
        if not valid_numbers:
            print("‚ùå No valid numbers found. Exiting.")
            sys.exit(1)
        
        # Convert to strings for namespace IDs
        namespace_ids = [str(num) for num in valid_numbers]
    
    # Execute batch crawling
    batch_crawl(
        namespace_ids,
        args.output_dir,
        args.workers
    )

if __name__ == '__main__':
    main()