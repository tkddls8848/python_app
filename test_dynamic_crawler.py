"""
ë™ì  í…Œì´ë¸” í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
https://www.data.go.kr/data/15001700/openapi.do í˜ì´ì§€ í¬ë¡¤ë§
"""
import asyncio
import json
import sys
import os

# ê²½ë¡œ ì„¤ì •
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from nara_crawler.hybrid.dynamic_table_crawler import DynamicTableCrawler


async def test_specific_location():
    """íŠ¹ì • ìœ„ì¹˜ í…Œì´ë¸” ë¶„ì„"""
    print("=" * 60)
    print("ë™ì  í…Œì´ë¸” í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    crawler = DynamicTableCrawler()

    url = "https://www.data.go.kr/data/15001700/openapi.do"
    xpath = '/html/body/div[2]/div/div[2]/div/div[2]/div[4]/div[2]/div[1]/div/div[1]'

    print(f"\nURL: {url}")
    print(f"XPath: {xpath}")
    print("\ní˜ì´ì§€ ë¶„ì„ ì¤‘...")

    try:
        result = await crawler.crawl_specific_location(url, xpath)

        if result['success']:
            print("\nâœ… ë¶„ì„ ì„±ê³µ!")

            data = result['data']

            # ìš”ì†Œ ì •ë³´
            print(f"\nğŸ“ ìš”ì†Œ ì •ë³´:")
            print(f"  íƒœê·¸: {data['element_info']['tag']}")
            print(f"  í´ë˜ìŠ¤: {data['element_info']['class']}")
            print(f"  ID: {data['element_info']['id']}")

            # ì…€ë ‰íŠ¸ ë°•ìŠ¤ ì •ë³´
            if data.get('select_count', 0) > 0:
                print(f"\nğŸ›ï¸ ì…€ë ‰íŠ¸ ë°•ìŠ¤: {data['select_count']}ê°œ")

                for select_info in data['selects']:
                    print(f"\n  ì…€ë ‰íŠ¸ {select_info['index'] + 1}:")
                    print(f"    ID: {select_info['id']}")
                    print(f"    Name: {select_info['name']}")
                    print(f"    ì˜µì…˜: {len(select_info['options'])}ê°œ")

                    if select_info['options']:
                        print(f"\n    ì˜µì…˜ ëª©ë¡ (ì²˜ìŒ 5ê°œ):")
                        for i, option in enumerate(select_info['options'][:5]):
                            print(f"      {i+1}. {option['text']} (value: {option['value']})")

                        if len(select_info['options']) > 5:
                            print(f"      ... ì™¸ {len(select_info['options']) - 5}ê°œ")

                # ì…€ë ‰í„° ê¸°ë°˜ í¬ë¡¤ë§ ì œì•ˆ
                if data['selects']:
                    select_id = data['selects'][0]['id']
                    print(f"\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„: ì…€ë ‰í„°ë³„ í¬ë¡¤ë§")
                    print(f"   python test_dynamic_crawler_full.py --select-id {select_id}")

            # í…Œì´ë¸” ì •ë³´
            if data.get('table_count', 0) > 0:
                print(f"\nğŸ“Š í…Œì´ë¸”: {data['table_count']}ê°œ")

                for table_info in data['tables']:
                    table_data = table_info['data']
                    print(f"\n  í…Œì´ë¸” {table_info['index'] + 1}:")
                    print(f"    í—¤ë”: {table_data['headers']}")
                    print(f"    í–‰ ê°œìˆ˜: {len(table_data['rows'])}")

                    if table_data['rows']:
                        print(f"    ì²« í–‰: {table_data['rows'][0]}")

            # ê²°ê³¼ ì €ì¥
            output_file = '/home/user/python_app/dynamic_table_analysis.json'
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)

            print(f"\nğŸ’¾ ì „ì²´ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤:")
            print(f"   {output_file}")

        else:
            print("\nâŒ ë¶„ì„ ì‹¤íŒ¨")
            print(f"ì—ëŸ¬: {result['errors']}")

    except Exception as e:
        print(f"\nâŒ ì—ëŸ¬ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


async def test_selector_based_crawling(selector_css=None, selector_id=None):
    """ì…€ë ‰í„° ê¸°ë°˜ ì „ì²´ í¬ë¡¤ë§"""
    print("=" * 60)
    print("ì…€ë ‰í„° ê¸°ë°˜ ë™ì  í…Œì´ë¸” í¬ë¡¤ë§")
    print("=" * 60)

    crawler = DynamicTableCrawler()

    url = "https://www.data.go.kr/data/15001700/openapi.do"
    xpath = '/html/body/div[2]/div/div[2]/div/div[2]/div[4]/div[2]/div[1]/div/div[1]'

    if selector_id:
        selector_css = f"select#{selector_id}"

    if not selector_css:
        print("âš ï¸ ì…€ë ‰í„° CSSê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        print("ë¨¼ì € test_specific_location()ì„ ì‹¤í–‰í•˜ì—¬ ì…€ë ‰íŠ¸ IDë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return

    print(f"\nURL: {url}")
    print(f"ì…€ë ‰í„°: {selector_css}")
    print(f"í…Œì´ë¸” ìœ„ì¹˜: {xpath}")
    print("\ní¬ë¡¤ë§ ì¤‘...")

    try:
        result = await crawler.crawl_dynamic_selector_table(
            url=url,
            selector_css=selector_css,
            table_container_xpath=xpath,
            wait_after_select=2.0
        )

        if result['success']:
            print("\nâœ… í¬ë¡¤ë§ ì„±ê³µ!")
            print(f"ìˆ˜ì§‘ëœ ì˜µì…˜: {len(result['data'])}ê°œ")

            for option_key, option_data in result['data'].items():
                print(f"\nğŸ“Œ {option_data['option_text']} (value: {option_key})")
                print(f"   í…Œì´ë¸”: {len(option_data['tables'])}ê°œ")

                for table in option_data['tables']:
                    table_data = table['data']
                    print(f"   - í…Œì´ë¸” {table['table_index'] + 1}: {len(table_data['rows'])}í–‰")

            # ê²°ê³¼ ì €ì¥
            output_file = '/home/user/python_app/dynamic_table_full_result.json'
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)

            print(f"\nğŸ’¾ ì „ì²´ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤:")
            print(f"   {output_file}")

        else:
            print("\nâŒ í¬ë¡¤ë§ ì‹¤íŒ¨")
            print(f"ì—ëŸ¬: {result['errors']}")

    except Exception as e:
        print(f"\nâŒ ì—ëŸ¬ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description='ë™ì  í…Œì´ë¸” í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸')
    parser.add_argument('--mode', choices=['analyze', 'crawl'], default='analyze',
                       help='ì‹¤í–‰ ëª¨ë“œ: analyze (ë¶„ì„) ë˜ëŠ” crawl (ì „ì²´ í¬ë¡¤ë§)')
    parser.add_argument('--select-id', help='ì…€ë ‰íŠ¸ ë°•ìŠ¤ ID (crawl ëª¨ë“œì—ì„œ í•„ìš”)')
    parser.add_argument('--select-css', help='ì…€ë ‰íŠ¸ ë°•ìŠ¤ CSS ì…€ë ‰í„°')

    args = parser.parse_args()

    if args.mode == 'analyze':
        asyncio.run(test_specific_location())
    elif args.mode == 'crawl':
        asyncio.run(test_selector_based_crawling(
            selector_css=args.select_css,
            selector_id=args.select_id
        ))
